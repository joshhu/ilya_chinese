{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 13：注意力就是你所需要的（Attention Is All You Need）\n",
    "## Vaswani 等人（2017）\n",
    "\n",
    "### Transformer：純注意力架構\n",
    "\n",
    "革命性的架構，用自注意力取代了 RNN，實現了現代大型語言模型（LLM）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 縮放點積注意力（Scaled Dot-Product Attention）\n",
    "\n",
    "基本構建塊：\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"數值穩定的 softmax\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    縮放點積注意力\n",
    "    \n",
    "    Q：查詢（Queries）(seq_len_q, d_k)\n",
    "    K：鍵（Keys）(seq_len_k, d_k)\n",
    "    V：值（Values）(seq_len_v, d_v)\n",
    "    mask：可選的遮罩 (seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # 計算注意力分數\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # 如果提供了遮罩，則應用遮罩（用於因果關係或填充）\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    \n",
    "    # Softmax 獲得注意力權重\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # 值的加權和\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 測試縮放點積注意力\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"注意力輸出形狀：{output.shape}\")\n",
    "print(f\"注意力權重形狀：{attn_weights.shape}\")\n",
    "print(f\"注意力權重總和（應為 1）：{attn_weights.sum(axis=1)}\")\n",
    "\n",
    "# 視覺化注意力模式\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attn_weights, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='注意力權重')\n",
    "plt.xlabel('鍵位置')\n",
    "plt.ylabel('查詢位置')\n",
    "plt.title('注意力權重矩陣')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多頭注意力（Multi-Head Attention）\n",
    "\n",
    "多個注意力「頭」關注輸入的不同方面：\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1, ..., head_h)W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # 所有頭的 Q、K、V 線性投影（並行化）\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.1\n",
    "        \n",
    "        # 輸出投影\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.1\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"分成多個頭：(seq_len, d_model) -> (num_heads, seq_len, d_k)\"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"合併頭：(num_heads, seq_len, d_k) -> (seq_len, d_model)\"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        x = x.transpose(1, 0, 2)\n",
    "        return x.reshape(seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        多頭注意力前向傳遞\n",
    "        \n",
    "        Q, K, V：(seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 線性投影\n",
    "        Q = np.dot(Q, self.W_q.T)\n",
    "        K = np.dot(K, self.W_k.T)\n",
    "        V = np.dot(V, self.W_v.T)\n",
    "        \n",
    "        # 分成多個頭\n",
    "        Q = self.split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # 對每個頭應用注意力\n",
    "        head_outputs = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            head_out, head_attn = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            head_outputs.append(head_out)\n",
    "            self.attention_weights.append(head_attn)\n",
    "        \n",
    "        # 堆疊頭\n",
    "        heads = np.stack(head_outputs, axis=0)  # (num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 合併頭\n",
    "        combined = self.combine_heads(heads)  # (seq_len, d_model)\n",
    "        \n",
    "        # 最終線性投影\n",
    "        output = np.dot(combined, self.W_o.T)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 測試多頭注意力\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "output = mha.forward(X, X, X)  # 自注意力\n",
    "\n",
    "print(f\"\\n多頭注意力：\")\n",
    "print(f\"輸入形狀：{X.shape}\")\n",
    "print(f\"輸出形狀：{output.shape}\")\n",
    "print(f\"頭數：{num_heads}\")\n",
    "print(f\"每個頭的維度：{mha.d_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置編碼（Positional Encoding）\n",
    "\n",
    "由於 Transformer 沒有遞迴，我們添加位置資訊：\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    創建正弦位置編碼\n",
    "    \"\"\"\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    position = np.arange(0, seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    # 對偶數索引應用 sin\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    \n",
    "    # 對奇數索引應用 cos\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# 生成位置編碼\n",
    "seq_len = 50\n",
    "d_model = 64\n",
    "pe = positional_encoding(seq_len, d_model)\n",
    "\n",
    "# 視覺化位置編碼\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(pe.T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='編碼值')\n",
    "plt.xlabel('位置')\n",
    "plt.ylabel('維度')\n",
    "plt.title('位置編碼（所有維度）')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# 繪製前幾個維度\n",
    "for i in [0, 1, 2, 3, 10, 20]:\n",
    "    plt.plot(pe[:, i], label=f'維度 {i}')\n",
    "plt.xlabel('位置')\n",
    "plt.ylabel('編碼值')\n",
    "plt.title('位置編碼（選定維度）')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"位置編碼形狀：{pe.shape}\")\n",
    "print(f\"不同頻率在不同尺度上編碼位置\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前饋網路（Feed-Forward Network）\n",
    "\n",
    "獨立應用於每個位置：\n",
    "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) * 0.1\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) * 0.1\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 帶 ReLU 的第一層\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        \n",
    "        # 第二層\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 測試前饋網路\n",
    "d_model = 64\n",
    "d_ff = 256  # 通常是 4 倍大\n",
    "\n",
    "ff = FeedForward(d_model, d_ff)\n",
    "x = np.random.randn(10, d_model)\n",
    "output = ff.forward(x)\n",
    "\n",
    "print(f\"\\n前饋網路：\")\n",
    "print(f\"輸入：{x.shape}\")\n",
    "print(f\"隱藏層：({x.shape[0]}, {d_ff})\")\n",
    "print(f\"輸出：{output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 層正規化（Layer Normalization）\n",
    "\n",
    "跨特徵正規化（不像 BatchNorm 那樣跨批次）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        \n",
    "        normalized = (x - mean) / (std + self.eps)\n",
    "        output = self.gamma * normalized + self.beta\n",
    "        \n",
    "        return output\n",
    "\n",
    "ln = LayerNorm(d_model)\n",
    "x = np.random.randn(10, d_model) * 3 + 5  # 未正規化的\n",
    "normalized = ln.forward(x)\n",
    "\n",
    "print(f\"\\n層正規化：\")\n",
    "print(f\"輸入均值：{x.mean():.4f}，標準差：{x.std():.4f}\")\n",
    "print(f\"輸出均值：{normalized.mean():.4f}，標準差：{normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的 Transformer 區塊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 帶殘差連接的多頭注意力\n",
    "        attn_output = self.attention.forward(x, x, x, mask)\n",
    "        x = self.norm1.forward(x + attn_output)\n",
    "        \n",
    "        # 帶殘差連接的前饋網路\n",
    "        ff_output = self.ff.forward(x)\n",
    "        x = self.norm2.forward(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 測試 Transformer 區塊\n",
    "block = TransformerBlock(d_model=64, num_heads=8, d_ff=256)\n",
    "x = np.random.randn(10, 64)\n",
    "output = block.forward(x)\n",
    "\n",
    "print(f\"\\nTransformer 區塊：\")\n",
    "print(f\"輸入形狀：{x.shape}\")\n",
    "print(f\"輸出形狀：{output.shape}\")\n",
    "print(f\"\\n區塊包含：\")\n",
    "print(f\"  1. 多頭自注意力\")\n",
    "print(f\"  2. 層正規化\")\n",
    "print(f\"  3. 前饋網路\")\n",
    "print(f\"  4. 殘差連接\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化多頭注意力模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建具有可解釋輸入的注意力\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "output = mha.forward(X, X, X)\n",
    "\n",
    "# 繪製每個頭的注意力模式\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    attn = mha.attention_weights[i]\n",
    "    im = ax.imshow(attn, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "    ax.set_title(f'頭 {i+1}')\n",
    "    ax.set_xlabel('鍵')\n",
    "    ax.set_ylabel('查詢')\n",
    "    \n",
    "plt.colorbar(im, ax=axes, label='注意力權重', fraction=0.046, pad=0.04)\n",
    "plt.suptitle('多頭注意力模式', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n每個頭學習關注不同的模式！\")\n",
    "print(\"不同的頭捕捉資料中不同的關係。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自迴歸模型的因果（遮罩）自注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"創建遮罩以防止關注未來位置\"\"\"\n",
    "    mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "    return mask\n",
    "\n",
    "# 測試因果注意力\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# 無遮罩（雙向）\n",
    "output_bi, attn_bi = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# 有因果遮罩（單向）\n",
    "output_causal, attn_causal = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "# 視覺化差異\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 因果遮罩\n",
    "ax1.imshow(causal_mask, cmap='Reds', aspect='auto')\n",
    "ax1.set_title('因果遮罩\\n（1 = 被遮罩/不允許）')\n",
    "ax1.set_xlabel('鍵位置')\n",
    "ax1.set_ylabel('查詢位置')\n",
    "\n",
    "# 雙向注意力\n",
    "im2 = ax2.imshow(attn_bi, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_title('雙向注意力\\n（可以看到未來）')\n",
    "ax2.set_xlabel('鍵位置')\n",
    "ax2.set_ylabel('查詢位置')\n",
    "\n",
    "# 因果注意力\n",
    "im3 = ax3.imshow(attn_causal, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax3.set_title('因果注意力\\n（看不到未來）')\n",
    "ax3.set_xlabel('鍵位置')\n",
    "ax3.set_ylabel('查詢位置')\n",
    "\n",
    "plt.colorbar(im3, ax=[ax2, ax3], label='注意力權重')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n因果遮罩對以下情況至關重要：\")\n",
    "print(\"  - 自迴歸生成（GPT、語言模型）\")\n",
    "print(\"  - 防止來自未來詞元的資訊洩漏\")\n",
    "print(\"  - 每個位置只能關注自己和之前的位置\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "### 為什麼「注意力就是你所需要的」？\n",
    "- **無遞迴**：並行處理整個序列\n",
    "- **無卷積**：純注意力機制\n",
    "- **更好的擴展性**：O(n²d) vs RNN 中的 O(n) 順序操作\n",
    "- **長距離依賴**：任意位置之間的直接連接\n",
    "\n",
    "### 核心組件：\n",
    "1. **縮放點積注意力**：高效的注意力計算\n",
    "2. **多頭注意力**：多個表示子空間\n",
    "3. **位置編碼**：注入位置資訊\n",
    "4. **前饋網路**：位置級變換\n",
    "5. **層正規化**：穩定訓練\n",
    "6. **殘差連接**：實現深度網路\n",
    "\n",
    "### 架構變體：\n",
    "- **編碼器-解碼器**：原始 Transformer（翻譯）\n",
    "- **僅編碼器**：BERT（雙向理解）\n",
    "- **僅解碼器**：GPT（自迴歸生成）\n",
    "\n",
    "### 優點：\n",
    "- 可並行化訓練（不像 RNN）\n",
    "- 更好的長距離依賴\n",
    "- 可解釋的注意力模式\n",
    "- 在許多任務上達到最先進水平\n",
    "\n",
    "### 影響：\n",
    "- 現代 NLP 的基礎：GPT、BERT、T5 等\n",
    "- 擴展到視覺：Vision Transformer（ViT）\n",
    "- 多模態模型：CLIP、Flamingo\n",
    "- 實現了數十億參數的大型語言模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
