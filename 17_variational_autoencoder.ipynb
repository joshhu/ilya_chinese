{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 17：變分有損自編碼器\n",
    "## Xi Chen, Diederik P. Kingma 等人 (2016)\n",
    "\n",
    "### VAE：具有學習潛在空間的生成模型\n",
    "\n",
    "結合深度學習與變分推斷進行生成建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 變分自編碼器（VAE）基礎\n",
    "\n",
    "VAE 學習：\n",
    "- **編碼器**：q(z|x) - 近似後驗\n",
    "- **解碼器**：p(x|z) - 生成模型\n",
    "\n",
    "**損失**：ELBO = 重建損失 + KL 散度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # 編碼器：x -> h -> (mu, log_var)\n",
    "        self.W_enc_h = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        self.b_enc_h = np.zeros(hidden_dim)\n",
    "        \n",
    "        self.W_mu = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
    "        self.b_mu = np.zeros(latent_dim)\n",
    "        \n",
    "        self.W_logvar = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
    "        self.b_logvar = np.zeros(latent_dim)\n",
    "        \n",
    "        # 解碼器：z -> h -> x_recon\n",
    "        self.W_dec_h = np.random.randn(latent_dim, hidden_dim) * 0.1\n",
    "        self.b_dec_h = np.zeros(hidden_dim)\n",
    "        \n",
    "        self.W_recon = np.random.randn(hidden_dim, input_dim) * 0.1\n",
    "        self.b_recon = np.zeros(input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        將輸入編碼為潛在分佈參數\n",
    "        \n",
    "        返回：q(z|x) 的 mu, log_var\n",
    "        \"\"\"\n",
    "        h = relu(np.dot(x, self.W_enc_h) + self.b_enc_h)\n",
    "        mu = np.dot(h, self.W_mu) + self.b_mu\n",
    "        log_var = np.dot(h, self.W_logvar) + self.b_logvar\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        重參數化技巧：z = mu + sigma * epsilon\n",
    "        其中 epsilon ~ N(0, I)\n",
    "        \"\"\"\n",
    "        std = np.exp(0.5 * log_var)\n",
    "        epsilon = np.random.randn(*mu.shape)\n",
    "        z = mu + std * epsilon\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        將潛在編碼解碼為重建\n",
    "        \n",
    "        返回：重建的 x\n",
    "        \"\"\"\n",
    "        h = relu(np.dot(z, self.W_dec_h) + self.b_dec_h)\n",
    "        x_recon = sigmoid(np.dot(h, self.W_recon) + self.b_recon)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        完整的前向傳遞\n",
    "        \"\"\"\n",
    "        # 編碼\n",
    "        mu, log_var = self.encode(x)\n",
    "        \n",
    "        # 採樣潛在變數\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # 解碼\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        return x_recon, mu, log_var, z\n",
    "    \n",
    "    def loss(self, x, x_recon, mu, log_var):\n",
    "        \"\"\"\n",
    "        VAE 損失 = 重建損失 + KL 散度\n",
    "        \"\"\"\n",
    "        # 重建損失（二元交叉熵）\n",
    "        recon_loss = -np.sum(\n",
    "            x * np.log(x_recon + 1e-8) + \n",
    "            (1 - x) * np.log(1 - x_recon + 1e-8)\n",
    "        )\n",
    "        \n",
    "        # KL 散度：KL(q(z|x) || p(z))\n",
    "        # 其中 p(z) = N(0, I)\n",
    "        # KL = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        kl_loss = -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var))\n",
    "        \n",
    "        return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# 建立 VAE\n",
    "input_dim = 16  # 例如展平的 4x4 圖像\n",
    "hidden_dim = 32\n",
    "latent_dim = 2  # 2D 用於視覺化\n",
    "\n",
    "vae = VAE(input_dim, hidden_dim, latent_dim)\n",
    "print(f\"VAE 已建立：\")\n",
    "print(f\"  輸入：{input_dim}\")\n",
    "print(f\"  隱藏：{hidden_dim}\")\n",
    "print(f\"  潛在：{latent_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成合成資料\n",
    "\n",
    "用於展示的簡單 4x4 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patterns(num_samples=100):\n",
    "    \"\"\"\n",
    "    生成簡單的 4x4 二元模式\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        pattern = np.zeros((4, 4))\n",
    "        \n",
    "        if i % 4 == 0:\n",
    "            # 水平線\n",
    "            pattern[1:2, :] = 1\n",
    "        elif i % 4 == 1:\n",
    "            # 垂直線\n",
    "            pattern[:, 2:3] = 1\n",
    "        elif i % 4 == 2:\n",
    "            # 對角線\n",
    "            np.fill_diagonal(pattern, 1)\n",
    "        else:\n",
    "            # 角落正方形\n",
    "            pattern[:2, :2] = 1\n",
    "        \n",
    "        # 添加小噪聲\n",
    "        noise = np.random.randn(4, 4) * 0.05\n",
    "        pattern = np.clip(pattern + noise, 0, 1)\n",
    "        \n",
    "        data.append(pattern.flatten())\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "# 生成訓練資料\n",
    "X_train = generate_patterns(200)\n",
    "\n",
    "# 視覺化樣本\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'模式 {i}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('訓練資料樣本')\n",
    "plt.show()\n",
    "\n",
    "print(f\"生成了 {len(X_train)} 個訓練樣本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試前向傳遞和損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在單一範例上測試\n",
    "x = X_train[0:1]\n",
    "x_recon, mu, log_var, z = vae.forward(x)\n",
    "\n",
    "total_loss, recon_loss, kl_loss = vae.loss(x, x_recon, mu, log_var)\n",
    "\n",
    "print(f\"前向傳遞：\")\n",
    "print(f\"  輸入形狀：{x.shape}\")\n",
    "print(f\"  潛在 mu：{mu}\")\n",
    "print(f\"  潛在 log_var：{log_var}\")\n",
    "print(f\"  潛在 z：{z}\")\n",
    "print(f\"  重建形狀：{x_recon.shape}\")\n",
    "print(f\"\\n損失：\")\n",
    "print(f\"  總計：{total_loss:.4f}\")\n",
    "print(f\"  重建：{recon_loss:.4f}\")\n",
    "print(f\"  KL 散度：{kl_loss:.4f}\")\n",
    "\n",
    "# 視覺化重建\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1.imshow(x.reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "ax1.set_title('原始')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(x_recon.reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "ax2.set_title('重建（未訓練）')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化潛在空間\n",
    "\n",
    "由於 latent_dim=2，我們可以視覺化學習到的表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編碼所有訓練資料\n",
    "latent_codes = []\n",
    "pattern_types = []\n",
    "\n",
    "for i, x in enumerate(X_train):\n",
    "    mu, log_var = vae.encode(x.reshape(1, -1))\n",
    "    latent_codes.append(mu[0])\n",
    "    pattern_types.append(i % 4)\n",
    "\n",
    "latent_codes = np.array(latent_codes)\n",
    "pattern_types = np.array(pattern_types)\n",
    "\n",
    "# 繪製潛在空間\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    latent_codes[:, 0], \n",
    "    latent_codes[:, 1], \n",
    "    c=pattern_types, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "plt.colorbar(scatter, label='模式類型')\n",
    "plt.xlabel('潛在維度 1')\n",
    "plt.ylabel('潛在維度 2')\n",
    "plt.title('潛在空間（未訓練的 VAE）')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"潛在空間視覺化顯示編碼模式的分佈\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 從先驗採樣並生成\n",
    "\n",
    "採樣 z ~ N(0, I) 並解碼以生成新樣本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從標準正態先驗採樣\n",
    "num_samples = 8\n",
    "z_samples = np.random.randn(num_samples, latent_dim)\n",
    "\n",
    "# 生成樣本\n",
    "generated = []\n",
    "for z in z_samples:\n",
    "    x_gen = vae.decode(z.reshape(1, -1))\n",
    "    generated.append(x_gen[0])\n",
    "\n",
    "# 視覺化生成的樣本\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(generated[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'z={z_samples[i][:2]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('從先驗 p(z) = N(0, I) 生成的樣本', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 潛在空間中的插值\n",
    "\n",
    "在潛在空間中兩點之間平滑插值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編碼兩個不同的模式\n",
    "x1 = X_train[0:1]  # 模式類型 0\n",
    "x2 = X_train[1:2]  # 模式類型 1\n",
    "\n",
    "mu1, _ = vae.encode(x1)\n",
    "mu2, _ = vae.encode(x2)\n",
    "\n",
    "# 插值\n",
    "num_steps = 8\n",
    "interpolated = []\n",
    "\n",
    "for alpha in np.linspace(0, 1, num_steps):\n",
    "    z_interp = (1 - alpha) * mu1 + alpha * mu2\n",
    "    x_interp = vae.decode(z_interp)\n",
    "    interpolated.append(x_interp[0])\n",
    "\n",
    "# 視覺化插值\n",
    "fig, axes = plt.subplots(1, num_steps, figsize=(16, 2))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(interpolated[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'α={i/(num_steps-1):.2f}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('潛在空間插值', fontsize=14, y=1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"平滑過渡顯示潛在空間的連續性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重參數化技巧視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示從同一分佈的多次採樣\n",
    "x = X_train[0:1]\n",
    "mu, log_var = vae.encode(x)\n",
    "\n",
    "# 多次採樣\n",
    "num_samples = 100\n",
    "z_samples = []\n",
    "for _ in range(num_samples):\n",
    "    z = vae.reparameterize(mu, log_var)\n",
    "    z_samples.append(z[0])\n",
    "\n",
    "z_samples = np.array(z_samples)\n",
    "\n",
    "# 繪製分佈\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1], alpha=0.3, s=20)\n",
    "plt.scatter(mu[0, 0], mu[0, 1], color='red', s=200, marker='*', label='μ', zorder=5)\n",
    "\n",
    "# 繪製 2 個標準差的橢圓\n",
    "std = np.exp(0.5 * log_var[0])\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "ellipse_x = mu[0, 0] + 2 * std[0] * np.cos(theta)\n",
    "ellipse_y = mu[0, 1] + 2 * std[1] * np.sin(theta)\n",
    "plt.plot(ellipse_x, ellipse_y, 'r--', label='2σ 邊界', linewidth=2)\n",
    "\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('重參數化技巧：z = μ + σ ⊙ ε，其中 ε ~ N(0,I)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"μ = {mu[0]}\")\n",
    "print(f\"σ = {std}\")\n",
    "print(f\"樣本均值：{z_samples.mean(axis=0)}\")\n",
    "print(f\"樣本標準差：{z_samples.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "### VAE 架構：\n",
    "1. **編碼器**：q_φ(z|x) - 將輸入映射到潛在分佈\n",
    "2. **重參數化**：z = μ + σ ⊙ ε（使反向傳播可行）\n",
    "3. **解碼器**：p_θ(x|z) - 從潛在編碼生成輸出\n",
    "\n",
    "### 損失函數（ELBO）：\n",
    "```\n",
    "L = E[log p(x|z)] - KL(q(z|x) || p(z))\n",
    "  = 重建損失 - KL 散度\n",
    "```\n",
    "\n",
    "### KL 散度：\n",
    "- 正則化潛在空間使其接近先驗 p(z) = N(0, I)\n",
    "- 防止過擬合\n",
    "- 確保潛在空間平滑\n",
    "\n",
    "### 重參數化技巧：\n",
    "- 使採樣可微分\n",
    "- z = μ(x) + σ(x) ⊙ ε，其中 ε ~ N(0, I)\n",
    "- 梯度通過 μ 和 σ 流動\n",
    "\n",
    "### 特性：\n",
    "- **生成式**：可以採樣新資料\n",
    "- **連續潛在空間**：平滑的插值\n",
    "- **機率式**：建模不確定性\n",
    "- **解糾纏表示**：（使用 β-VAE 等）\n",
    "\n",
    "### 應用：\n",
    "- 圖像生成\n",
    "- 降維\n",
    "- 半監督學習\n",
    "- 異常檢測\n",
    "- 資料增強\n",
    "\n",
    "### 變體：\n",
    "- **β-VAE**：加權 KL 以實現解糾纏\n",
    "- **條件 VAE**：條件生成\n",
    "- **階層式 VAE**：多層潛在層\n",
    "- **VQ-VAE**：離散潛在變數"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
