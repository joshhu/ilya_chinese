{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 15：深度殘差網路中的恆等映射\n",
    "## Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2016)\n",
    "\n",
    "### 預激活 ResNet\n",
    "\n",
    "改進的殘差區塊，具有更好的梯度流。關鍵洞察：將激活移到卷積**之前**！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始 ResNet 區塊\n",
    "\n",
    "```\n",
    "x → Conv → BN → ReLU → Conv → BN → (+) → ReLU → output\n",
    "    ↓                                  ↑\n",
    "    └──────────── identity ────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def batch_norm_1d(x, gamma=1.0, beta=0.0, eps=1e-5):\n",
    "    \"\"\"簡化的一維批次正規化\"\"\"\n",
    "    mean = np.mean(x)\n",
    "    var = np.var(x)\n",
    "    x_normalized = (x - mean) / np.sqrt(var + eps)\n",
    "    return gamma * x_normalized + beta\n",
    "\n",
    "class OriginalResidualBlock:\n",
    "    \"\"\"原始 ResNet 區塊（後激活）\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        # 兩層\n",
    "        self.W1 = np.random.randn(dim, dim) * 0.01\n",
    "        self.W2 = np.random.randn(dim, dim) * 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        原始：x → Conv → BN → ReLU → Conv → BN → (+x) → ReLU\n",
    "        \"\"\"\n",
    "        # 第一個 conv-bn-relu\n",
    "        out = np.dot(self.W1, x)\n",
    "        out = batch_norm_1d(out)\n",
    "        out = relu(out)\n",
    "        \n",
    "        # 第二個 conv-bn\n",
    "        out = np.dot(self.W2, out)\n",
    "        out = batch_norm_1d(out)\n",
    "        \n",
    "        # 加上恆等（殘差連接）\n",
    "        out = out + x\n",
    "        \n",
    "        # 最後的 ReLU（後激活）\n",
    "        out = relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 測試\n",
    "original_block = OriginalResidualBlock(dim=8)\n",
    "x = np.random.randn(8)\n",
    "output_original = original_block.forward(x)\n",
    "\n",
    "print(f\"輸入：{x[:4]}...\")\n",
    "print(f\"原始 ResNet 輸出：{output_original[:4]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預激活 ResNet 區塊\n",
    "\n",
    "```\n",
    "x → BN → ReLU → Conv → BN → ReLU → Conv → (+) → output\n",
    "    ↓                                       ↑\n",
    "    └──────────── identity ─────────────────┘\n",
    "```\n",
    "\n",
    "**關鍵差異**：激活在卷積**之前**，乾淨的恆等路徑！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActivationResidualBlock:\n",
    "    \"\"\"預激活 ResNet 區塊（改進版）\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.W1 = np.random.randn(dim, dim) * 0.01\n",
    "        self.W2 = np.random.randn(dim, dim) * 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        預激活：x → BN → ReLU → Conv → BN → ReLU → Conv → (+x)\n",
    "        \"\"\"\n",
    "        # 第一個 bn-relu-conv\n",
    "        out = batch_norm_1d(x)\n",
    "        out = relu(out)\n",
    "        out = np.dot(self.W1, out)\n",
    "        \n",
    "        # 第二個 bn-relu-conv\n",
    "        out = batch_norm_1d(out)\n",
    "        out = relu(out)\n",
    "        out = np.dot(self.W2, out)\n",
    "        \n",
    "        # 加上恆等（之後無激活！）\n",
    "        out = out + x\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 測試\n",
    "preact_block = PreActivationResidualBlock(dim=8)\n",
    "output_preact = preact_block.forward(x)\n",
    "\n",
    "print(f\"\\n預激活 ResNet 輸出：{output_preact[:4]}...\")\n",
    "print(f\"\\n關鍵差異：乾淨的恆等路徑（加法後無 ReLU）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度流分析\n",
    "\n",
    "為什麼預激活更好："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_flow(block_type, num_layers=10, input_dim=8):\n",
    "    \"\"\"\n",
    "    模擬梯度通過堆疊的殘差區塊的流動\n",
    "    \"\"\"\n",
    "    x = np.random.randn(input_dim)\n",
    "    \n",
    "    # 建立區塊\n",
    "    if block_type == 'original':\n",
    "        blocks = [OriginalResidualBlock(input_dim) for _ in range(num_layers)]\n",
    "    else:\n",
    "        blocks = [PreActivationResidualBlock(input_dim) for _ in range(num_layers)]\n",
    "    \n",
    "    # 前向傳遞\n",
    "    activations = [x]\n",
    "    current = x\n",
    "    for block in blocks:\n",
    "        current = block.forward(current)\n",
    "        activations.append(current.copy())\n",
    "    \n",
    "    # 模擬反向傳遞（簡化的梯度流）\n",
    "    grad = np.ones(input_dim)  # 來自損失的梯度\n",
    "    gradients = [grad]\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        # 對於殘差區塊：梯度分成恆等 + 殘差路徑\n",
    "        # 預激活有更乾淨的梯度流\n",
    "        \n",
    "        if block_type == 'original':\n",
    "            # 後激活：梯度受 ReLU 導數影響\n",
    "            # 簡化：部分梯度被 ReLU 殺死\n",
    "            grad_through_residual = grad * np.random.uniform(0.5, 1.0, input_dim)\n",
    "            grad = grad + grad_through_residual  # 恆等 + 殘差\n",
    "        else:\n",
    "            # 預激活：乾淨的恆等路徑\n",
    "            grad_through_residual = grad * np.random.uniform(0.7, 1.0, input_dim)\n",
    "            grad = grad + grad_through_residual  # 更好的梯度流\n",
    "        \n",
    "        gradients.append(grad.copy())\n",
    "    \n",
    "    return activations, gradients\n",
    "\n",
    "# 比較梯度流\n",
    "_, grad_original = compute_gradient_flow('original', num_layers=20)\n",
    "_, grad_preact = compute_gradient_flow('preact', num_layers=20)\n",
    "\n",
    "# 計算梯度大小\n",
    "grad_mag_original = [np.linalg.norm(g) for g in grad_original]\n",
    "grad_mag_preact = [np.linalg.norm(g) for g in grad_preact]\n",
    "\n",
    "# 繪圖\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(grad_mag_original, 'o-', label='原始 ResNet（後激活）', linewidth=2)\n",
    "plt.plot(grad_mag_preact, 's-', label='預激活 ResNet', linewidth=2)\n",
    "plt.xlabel('層（從輸出到輸入）', fontsize=12)\n",
    "plt.ylabel('梯度大小', fontsize=12)\n",
    "plt.title('梯度流比較', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"原始 ResNet 輸入處梯度：{grad_mag_original[-1]:.2f}\")\n",
    "print(f\"預激活輸入處梯度：{grad_mag_preact[-1]:.2f}\")\n",
    "print(f\"\\n預激活維持更強的梯度！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同的激活放置方式\n",
    "\n",
    "論文分析了各種放置選項："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化不同架構\n",
    "architectures = [\n",
    "    {\n",
    "        'name': '原始',\n",
    "        'structure': 'x → Conv → BN → ReLU → Conv → BN → (+x) → ReLU',\n",
    "        'identity': '被 ReLU 阻擋',\n",
    "        'score': '★★★☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': '加法後 BN',\n",
    "        'structure': 'x → Conv → BN → ReLU → Conv → BN → (+x) → BN → ReLU',\n",
    "        'identity': '被 BN & ReLU 阻擋',\n",
    "        'score': '★★☆☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': '加法前 ReLU',\n",
    "        'structure': 'x → BN → ReLU → Conv → BN → ReLU → Conv → ReLU → (+x)',\n",
    "        'identity': '被 ReLU 阻擋',\n",
    "        'score': '★★☆☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': '完全預激活',\n",
    "        'structure': 'x → BN → ReLU → Conv → BN → ReLU → Conv → (+x)',\n",
    "        'identity': '乾淨！✓',\n",
    "        'score': '★★★★★'\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"殘差區塊架構比較\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, arch in enumerate(architectures, 1):\n",
    "    print(f\"{i}. {arch['name']:20s} {arch['score']}\")\n",
    "    print(f\"   結構：{arch['structure']}\")\n",
    "    print(f\"   恆等路徑：{arch['identity']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"贏家：完全預激活（BN → ReLU → Conv）\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度網路比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResNet:\n",
    "    \"\"\"殘差區塊的堆疊\"\"\"\n",
    "    def __init__(self, dim, num_blocks, block_type='preact'):\n",
    "        self.blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            if block_type == 'preact':\n",
    "                self.blocks.append(PreActivationResidualBlock(dim))\n",
    "            else:\n",
    "                self.blocks.append(OriginalResidualBlock(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "            activations.append(x.copy())\n",
    "        return x, activations\n",
    "\n",
    "# 比較深度網路\n",
    "depth = 50\n",
    "dim = 16\n",
    "x_input = np.random.randn(dim)\n",
    "\n",
    "net_original = DeepResNet(dim, depth, 'original')\n",
    "net_preact = DeepResNet(dim, depth, 'preact')\n",
    "\n",
    "out_original, acts_original = net_original.forward(x_input)\n",
    "out_preact, acts_preact = net_preact.forward(x_input)\n",
    "\n",
    "# 計算激活統計\n",
    "norms_original = [np.linalg.norm(a) for a in acts_original]\n",
    "norms_preact = [np.linalg.norm(a) for a in acts_preact]\n",
    "\n",
    "# 繪製激活範數\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# 激活大小\n",
    "ax1.plot(norms_original, label='原始 ResNet', linewidth=2)\n",
    "ax1.plot(norms_preact, label='預激活 ResNet', linewidth=2)\n",
    "ax1.set_xlabel('層', fontsize=12)\n",
    "ax1.set_ylabel('激活大小', fontsize=12)\n",
    "ax1.set_title(f'激活流（深度={depth}）', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 激活熱圖\n",
    "acts_matrix_original = np.array(acts_original).T\n",
    "acts_matrix_preact = np.array(acts_preact).T\n",
    "\n",
    "im = ax2.imshow(acts_matrix_preact - acts_matrix_original, cmap='RdBu', aspect='auto')\n",
    "ax2.set_xlabel('層', fontsize=12)\n",
    "ax2.set_ylabel('特徵維度', fontsize=12)\n",
    "ax2.set_title('差異（預激活 - 原始）', fontsize=14)\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n原始 ResNet 最終範數：{norms_original[-1]:.4f}\")\n",
    "print(f\"預激活最終範數：{norms_preact[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恆等映射分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_identity_mapping(block, num_tests=100):\n",
    "    \"\"\"\n",
    "    測試區塊學習恆等映射的能力\n",
    "    （當殘差路徑學習零時，輸出應等於輸入）\n",
    "    \"\"\"\n",
    "    # 將權重歸零（殘差路徑不學習任何東西）\n",
    "    block.W1 = np.zeros_like(block.W1)\n",
    "    block.W2 = np.zeros_like(block.W2)\n",
    "    \n",
    "    errors = []\n",
    "    for _ in range(num_tests):\n",
    "        x = np.random.randn(block.dim)\n",
    "        y = block.forward(x)\n",
    "        error = np.linalg.norm(y - x)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.mean(errors), np.std(errors)\n",
    "\n",
    "# 測試兩種區塊類型\n",
    "original_test = OriginalResidualBlock(dim=8)\n",
    "preact_test = PreActivationResidualBlock(dim=8)\n",
    "\n",
    "mean_err_original, std_err_original = test_identity_mapping(original_test)\n",
    "mean_err_preact, std_err_preact = test_identity_mapping(preact_test)\n",
    "\n",
    "print(\"\\n恆等映射測試（殘差路徑 = 0）：\")\n",
    "print(\"=\"*60)\n",
    "print(f\"原始 ResNet 誤差：{mean_err_original:.6f} ± {std_err_original:.6f}\")\n",
    "print(f\"預激活誤差：      {mean_err_preact:.6f} ± {std_err_preact:.6f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n預激活有{'更好' if mean_err_preact < mean_err_original else '更差'}的恆等映射！\")\n",
    "print(\"（較低誤差 = 更乾淨的恆等路徑）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化架構比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立視覺比較\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "def draw_block(ax, title, is_preact=False):\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 12)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # 恆等路徑（左）\n",
    "    ax.plot([1, 1], [1, 11], 'b-', linewidth=4, label='恆等路徑')\n",
    "    ax.arrow(1, 10.5, 0, -0.3, head_width=0.3, head_length=0.2, fc='blue', ec='blue')\n",
    "    \n",
    "    # 殘差路徑（右）\n",
    "    y_pos = 11\n",
    "    \n",
    "    if is_preact:\n",
    "        # 預激活：BN → ReLU → Conv → BN → ReLU → Conv\n",
    "        operations = ['BN', 'ReLU', 'Conv', 'BN', 'ReLU', 'Conv']\n",
    "        colors = ['lightgreen', 'lightyellow', 'lightblue', 'lightgreen', 'lightyellow', 'lightblue']\n",
    "    else:\n",
    "        # 原始：Conv → BN → ReLU → Conv → BN\n",
    "        operations = ['Conv', 'BN', 'ReLU', 'Conv', 'BN', 'ReLU*']\n",
    "        colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightblue', 'lightgreen', 'lightcoral']\n",
    "    \n",
    "    for i, (op, color) in enumerate(zip(operations, colors)):\n",
    "        y = y_pos - i * 1.5\n",
    "        \n",
    "        # 繪製方框\n",
    "        width = 2\n",
    "        height = 1\n",
    "        ax.add_patch(plt.Rectangle((6-width/2, y-height/2), width, height, \n",
    "                                   fill=True, color=color, ec='black', linewidth=2))\n",
    "        ax.text(6, y, op, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # 繪製到下一個的箭頭\n",
    "        if i < len(operations) - 1:\n",
    "            ax.arrow(6, y-height/2-0.1, 0, -0.3, head_width=0.2, head_length=0.1, \n",
    "                    fc='black', ec='black', linewidth=1.5)\n",
    "    \n",
    "    # 加法\n",
    "    add_y = y_pos - len(operations) * 1.5\n",
    "    ax.plot([1, 6], [add_y, add_y], 'k-', linewidth=2)\n",
    "    ax.scatter([3.5], [add_y], s=500, c='white', edgecolors='black', linewidths=3, zorder=5)\n",
    "    ax.text(3.5, add_y, '+', ha='center', va='center', fontsize=20, fontweight='bold', zorder=6)\n",
    "    \n",
    "    # 輸出箭頭\n",
    "    ax.arrow(3.5, add_y-0.3, 0, -0.5, head_width=0.3, head_length=0.2, \n",
    "            fc='green', ec='green', linewidth=3)\n",
    "    ax.text(3.5, add_y-1.2, '輸出', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 輸入\n",
    "    ax.text(1, 11.5, '輸入', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(6, 11.5, '輸入', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 註解\n",
    "    if not is_preact:\n",
    "        ax.text(8.5, add_y, 'ReLU* 阻擋\\n恆等！', fontsize=10, color='red', \n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    else:\n",
    "        ax.text(8.5, add_y, '乾淨的\\n恆等！', fontsize=10, color='green',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "draw_block(axes[0], '原始 ResNet（後激活）', is_preact=False)\n",
    "draw_block(axes[1], '預激活 ResNet（改進版）', is_preact=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "### 恆等映射問題：\n",
    "\n",
    "在原始 ResNet 中：\n",
    "```\n",
    "y = ReLU(F(x) + x)\n",
    "```\n",
    "加法**之後的 ReLU 阻擋**了恆等路徑！\n",
    "\n",
    "### 預激活解決方案：\n",
    "\n",
    "```\n",
    "y = F'(x) + x\n",
    "```\n",
    "其中 F'(x) = Conv(ReLU(BN(Conv(ReLU(BN(x))))))\n",
    "\n",
    "**乾淨的恆等路徑** → 更好的梯度流！\n",
    "\n",
    "### 關鍵改變：\n",
    "\n",
    "1. **將 BN 移到 Conv 之前**：`x → BN → ReLU → Conv`\n",
    "2. **移除最後的 ReLU**：加法後無激活\n",
    "3. **結果**：恆等路徑真正是恆等\n",
    "\n",
    "### 梯度流：\n",
    "\n",
    "**原始**：\n",
    "```\n",
    "∂L/∂x = ∂L/∂y · (∂F/∂x + I) · ∂ReLU/∂y\n",
    "```\n",
    "ReLU 導數殺死梯度！\n",
    "\n",
    "**預激活**：\n",
    "```\n",
    "∂L/∂x = ∂L/∂y · (∂F'/∂x + I)\n",
    "```\n",
    "乾淨的梯度通過恆等流動！\n",
    "\n",
    "### 優點：\n",
    "\n",
    "- ✅ **更好的梯度流**：恆等路徑上無阻擋\n",
    "- ✅ **更容易優化**：可以訓練更深的網路（1000+ 層）\n",
    "- ✅ **更好的準確度**：小但一致的改進\n",
    "- ✅ **正則化**：Conv 之前的 BN 作為正則化器\n",
    "\n",
    "### 比較：\n",
    "\n",
    "| 架構 | 恆等路徑 | 梯度流 | 效能 |\n",
    "|------|---------|--------|------|\n",
    "| 原始 ResNet | 被 ReLU 阻擋 | 良好 | ★★★★☆ |\n",
    "| 預激活 | **乾淨** | **更好** | ★★★★★ |\n",
    "\n",
    "### 實作提示：\n",
    "\n",
    "1. 對非常深的網路（>50 層）使用預激活\n",
    "2. 對較淺的網路保留原始 ResNet（向後兼容）\n",
    "3. 第一層可以保持後激活（還沒有恆等）\n",
    "4. 最後一層需要後激活以獲得最終輸出\n",
    "\n",
    "### 結果：\n",
    "\n",
    "- CIFAR-10：成功訓練 1001 層網路！\n",
    "- ImageNet：比原始 ResNet 持續改進\n",
    "- 實現了 1000+ 層網路的訓練\n",
    "\n",
    "### 為什麼重要：\n",
    "\n",
    "這篇論文展示了**架構細節很重要**。小的改變（移動 BN/ReLU）可以對可訓練性和效能產生重大影響。這是深度學習研究中迭代改進的關鍵例子。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
