{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 14：透過聯合學習對齊與翻譯的神經機器翻譯\n",
    "## Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio (2014)\n",
    "\n",
    "### 原始注意力機制\n",
    "\n",
    "這篇論文引入了**注意力（Attention）**機制——深度學習中最重要的創新之一。它比 Transformers 早了 3 年！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題：固定長度的上下文向量\n",
    "\n",
    "傳統的 seq2seq 將整個輸入壓縮成單一向量 → 資訊瓶頸！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "class EncoderRNN:\n",
    "    \"\"\"雙向 RNN 編碼器\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 前向 RNN\n",
    "        self.W_fwd = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_fwd = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # 後向 RNN\n",
    "        self.W_bwd = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_bwd = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs：(input_size, 1) 向量的列表\n",
    "        返回：雙向隱藏狀態的列表 (2*hidden_size, 1)\n",
    "        \"\"\"\n",
    "        seq_len = len(inputs)\n",
    "        \n",
    "        # 前向傳遞\n",
    "        h_fwd = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for x in inputs:\n",
    "            concat = np.vstack([x, h])\n",
    "            h = np.tanh(np.dot(self.W_fwd, concat) + self.b_fwd)\n",
    "            h_fwd.append(h)\n",
    "        \n",
    "        # 後向傳遞\n",
    "        h_bwd = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for x in reversed(inputs):\n",
    "            concat = np.vstack([x, h])\n",
    "            h = np.tanh(np.dot(self.W_bwd, concat) + self.b_bwd)\n",
    "            h_bwd.append(h)\n",
    "        h_bwd = list(reversed(h_bwd))\n",
    "        \n",
    "        # 串接前向和後向\n",
    "        annotations = [np.vstack([h_f, h_b]) for h_f, h_b in zip(h_fwd, h_bwd)]\n",
    "        \n",
    "        return annotations\n",
    "\n",
    "print(\"雙向編碼器已建立\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau 注意力機制\n",
    "\n",
    "關鍵創新：聯合對齊與翻譯！\n",
    "\n",
    "**注意力分數**：$e_{ij} = a(s_{i-1}, h_j)$，其中 $s$ 是解碼器狀態，$h$ 是編碼器標註\n",
    "\n",
    "**注意力權重**：$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_k \\exp(e_{ik})}$\n",
    "\n",
    "**上下文向量**：$c_i = \\sum_j \\alpha_{ij} h_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention:\n",
    "    \"\"\"加性注意力機制\"\"\"\n",
    "    def __init__(self, hidden_size, annotation_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 注意力參數\n",
    "        self.W_a = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.U_a = np.random.randn(hidden_size, annotation_size) * 0.01\n",
    "        self.v_a = np.random.randn(1, hidden_size) * 0.01\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_annotations):\n",
    "        \"\"\"\n",
    "        decoder_hidden：(hidden_size, 1) - 當前解碼器狀態 s_{i-1}\n",
    "        encoder_annotations：(annotation_size, 1) 的列表 - 所有編碼器狀態 h_j\n",
    "        \n",
    "        返回：\n",
    "        context：(annotation_size, 1) - 標註的加權和\n",
    "        attention_weights：(seq_len,) - 注意力分佈\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # 計算每個位置的注意力分數\n",
    "        for h_j in encoder_annotations:\n",
    "            # e_ij = v_a^T * tanh(W_a * s_{i-1} + U_a * h_j)\n",
    "            score = np.dot(self.v_a, np.tanh(\n",
    "                np.dot(self.W_a, decoder_hidden) + \n",
    "                np.dot(self.U_a, h_j)\n",
    "            ))\n",
    "            scores.append(score[0, 0])\n",
    "        \n",
    "        # Softmax 得到注意力權重\n",
    "        scores = np.array(scores)\n",
    "        attention_weights = softmax(scores)\n",
    "        \n",
    "        # 計算上下文向量作為加權和\n",
    "        context = sum(alpha * h for alpha, h in zip(attention_weights, encoder_annotations))\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "print(\"Bahdanau 注意力機制已建立\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 帶注意力的解碼器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    \"\"\"帶 Bahdanau 注意力的 RNN 解碼器\"\"\"\n",
    "    def __init__(self, output_size, hidden_size, annotation_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 注意力機制\n",
    "        self.attention = BahdanauAttention(hidden_size, annotation_size)\n",
    "        \n",
    "        # RNN：接收前一個輸出 + 上下文\n",
    "        input_size = output_size + annotation_size\n",
    "        self.W_dec = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_dec = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # 輸出層\n",
    "        self.W_out = np.random.randn(output_size, hidden_size + annotation_size + output_size) * 0.01\n",
    "        self.b_out = np.zeros((output_size, 1))\n",
    "    \n",
    "    def step(self, prev_output, decoder_hidden, encoder_annotations):\n",
    "        \"\"\"\n",
    "        單一解碼步驟\n",
    "        \n",
    "        prev_output：(output_size, 1) - 前一個輸出詞\n",
    "        decoder_hidden：(hidden_size, 1) - 前一個解碼器狀態\n",
    "        encoder_annotations：(annotation_size, 1) 的列表 - 編碼器狀態\n",
    "        \n",
    "        返回：\n",
    "        output：(output_size, 1) - 預測的輸出分佈\n",
    "        new_hidden：(hidden_size, 1) - 新的解碼器狀態\n",
    "        attention_weights：注意力分佈\n",
    "        \"\"\"\n",
    "        # 計算注意力和上下文\n",
    "        context, attention_weights = self.attention.forward(decoder_hidden, encoder_annotations)\n",
    "        \n",
    "        # 解碼器 RNN：s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "        rnn_input = np.vstack([prev_output, context])\n",
    "        concat = np.vstack([rnn_input, decoder_hidden])\n",
    "        new_hidden = np.tanh(np.dot(self.W_dec, concat) + self.b_dec)\n",
    "        \n",
    "        # 輸出：y_i = g(s_i, y_{i-1}, c_i)\n",
    "        output_input = np.vstack([new_hidden, context, prev_output])\n",
    "        output = np.dot(self.W_out, output_input) + self.b_out\n",
    "        \n",
    "        return output, new_hidden, attention_weights\n",
    "    \n",
    "    def forward(self, encoder_annotations, max_length=20, start_token=None):\n",
    "        \"\"\"\n",
    "        完整解碼\n",
    "        \"\"\"\n",
    "        if start_token is None:\n",
    "            start_token = np.zeros((self.output_size, 1))\n",
    "        \n",
    "        outputs = []\n",
    "        attention_history = []\n",
    "        \n",
    "        # 初始化\n",
    "        decoder_hidden = np.zeros((self.hidden_size, 1))\n",
    "        prev_output = start_token\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, decoder_hidden, attention_weights = self.step(\n",
    "                prev_output, decoder_hidden, encoder_annotations\n",
    "            )\n",
    "            \n",
    "            outputs.append(output)\n",
    "            attention_history.append(attention_weights)\n",
    "            \n",
    "            # 下一個輸入是當前輸出（貪婪解碼）\n",
    "            prev_output = output\n",
    "        \n",
    "        return outputs, attention_history\n",
    "\n",
    "print(\"注意力解碼器已建立\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的帶注意力 Seq2Seq 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention:\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size=32):\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 嵌入層\n",
    "        self.input_embedding = np.random.randn(input_vocab_size, hidden_size) * 0.01\n",
    "        self.output_embedding = np.random.randn(output_vocab_size, hidden_size) * 0.01\n",
    "        \n",
    "        # 編碼器（雙向，所以標註大小是 2*hidden_size）\n",
    "        self.encoder = EncoderRNN(hidden_size, hidden_size)\n",
    "        \n",
    "        # 帶注意力的解碼器\n",
    "        annotation_size = 2 * hidden_size\n",
    "        self.decoder = AttentionDecoder(hidden_size, hidden_size, annotation_size)\n",
    "    \n",
    "    def translate(self, input_sequence, max_output_length=15):\n",
    "        \"\"\"\n",
    "        將輸入序列翻譯成輸出序列\n",
    "        \n",
    "        input_sequence：token 索引的列表\n",
    "        \"\"\"\n",
    "        # 嵌入輸入\n",
    "        embedded = [self.input_embedding[idx:idx+1].T for idx in input_sequence]\n",
    "        \n",
    "        # 編碼\n",
    "        annotations = self.encoder.forward(embedded)\n",
    "        \n",
    "        # 解碼\n",
    "        start_token = self.output_embedding[0:1].T  # 使用第一個 token 作為開始\n",
    "        outputs, attention_history = self.decoder.forward(\n",
    "            annotations, max_length=max_output_length, start_token=start_token\n",
    "        )\n",
    "        \n",
    "        return outputs, attention_history, annotations\n",
    "\n",
    "# 建立模型\n",
    "input_vocab_size = 20   # 源語言詞彙表\n",
    "output_vocab_size = 20  # 目標語言詞彙表\n",
    "model = Seq2SeqWithAttention(input_vocab_size, output_vocab_size, hidden_size=16)\n",
    "\n",
    "print(f\"帶注意力的 Seq2Seq 已建立\")\n",
    "print(f\"輸入詞彙表：{input_vocab_size}\")\n",
    "print(f\"輸出詞彙表：{output_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在合成翻譯任務上測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的合成任務：反轉序列\n",
    "# 輸入：[1, 2, 3, 4, 5]\n",
    "# 輸出：[5, 4, 3, 2, 1]\n",
    "\n",
    "input_seq = [1, 2, 3, 4, 5, 6, 7]\n",
    "outputs, attention_history, annotations = model.translate(input_seq, max_output_length=len(input_seq))\n",
    "\n",
    "print(f\"輸入序列：{input_seq}\")\n",
    "print(f\"輸出步數：{len(outputs)}\")\n",
    "print(f\"注意力分佈數：{len(attention_history)}\")\n",
    "print(f\"編碼器標註形狀：{len(annotations)} x {annotations[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化注意力權重\n",
    "\n",
    "關鍵洞察：看看模型注意什麼！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將注意力歷史轉換為矩陣\n",
    "attention_matrix = np.array(attention_history)  # (output_len, input_len)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap='Blues', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='注意力權重')\n",
    "plt.xlabel('輸入位置（源）')\n",
    "plt.ylabel('輸出位置（目標）')\n",
    "plt.title('Bahdanau 注意力對齊矩陣')\n",
    "\n",
    "# 添加網格\n",
    "plt.xticks(range(len(input_seq)), [f'x{i+1}' for i in input_seq])\n",
    "plt.yticks(range(len(outputs)), [f'y{i+1}' for i in range(len(outputs))])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n注意力模式顯示哪些輸入位置影響每個輸出。\")\n",
    "print(\"較亮的格子 = 較高的注意力權重。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每個解碼器步驟的注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化特定解碼器步驟的注意力分佈\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "steps_to_show = min(8, len(attention_history))\n",
    "\n",
    "for i in range(steps_to_show):\n",
    "    axes[i].bar(range(len(input_seq)), attention_history[i])\n",
    "    axes[i].set_title(f'輸出步驟 {i+1}')\n",
    "    axes[i].set_xlabel('輸入位置')\n",
    "    axes[i].set_ylabel('注意力權重')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].set_xticks(range(len(input_seq)))\n",
    "    axes[i].set_xticklabels([f'x{j+1}' for j in input_seq], fontsize=8)\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('每個解碼步驟的注意力分佈', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"每個解碼器步驟關注不同的輸入位置！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比較：有注意力 vs 無注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬固定上下文的 seq2seq（無注意力）\n",
    "def fixed_context_attention(seq_len):\n",
    "    \"\"\"模擬只注意最後一個編碼器狀態\"\"\"\n",
    "    weights = np.zeros(seq_len)\n",
    "    weights[-1] = 1.0  # 只注意最後一個位置\n",
    "    return weights\n",
    "\n",
    "# 建立比較\n",
    "input_length = len(input_seq)\n",
    "output_length = len(outputs)\n",
    "\n",
    "# 固定上下文\n",
    "fixed_attention = np.array([fixed_context_attention(input_length) for _ in range(output_length)])\n",
    "\n",
    "# 繪製比較圖\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 無注意力（固定上下文）\n",
    "im1 = ax1.imshow(fixed_attention, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "ax1.set_xlabel('輸入位置')\n",
    "ax1.set_ylabel('輸出位置')\n",
    "ax1.set_title('無注意力（固定上下文）\\n所有解碼器步驟只看最後一個編碼器狀態')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# 有 Bahdanau 注意力\n",
    "im2 = ax2.imshow(attention_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_xlabel('輸入位置')\n",
    "ax2.set_ylabel('輸出位置')\n",
    "ax2.set_title('有 Bahdanau 注意力\\n每個解碼器步驟注意不同位置')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n關鍵差異：\")\n",
    "print(\"  無注意力：資訊瓶頸在最後一個編碼器狀態\")\n",
    "print(\"  有注意力：動態存取所有編碼器狀態\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力機制的變體"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bahdanau_score(s, h, W_a, U_a, v_a):\n",
    "    \"\"\"加性/串接注意力（Bahdanau）\"\"\"\n",
    "    return np.dot(v_a.T, np.tanh(np.dot(W_a, s) + np.dot(U_a, h)))[0, 0]\n",
    "\n",
    "def dot_product_score(s, h):\n",
    "    \"\"\"點積注意力（Luong）\"\"\"\n",
    "    return np.dot(s.T, h)[0, 0]\n",
    "\n",
    "def scaled_dot_product_score(s, h):\n",
    "    \"\"\"縮放點積（Transformer 風格）\"\"\"\n",
    "    d_k = s.shape[0]\n",
    "    return np.dot(s.T, h)[0, 0] / np.sqrt(d_k)\n",
    "\n",
    "# 比較評分函數\n",
    "s = np.random.randn(16, 1)\n",
    "h = np.random.randn(32, 1)\n",
    "W_a = np.random.randn(16, 16)\n",
    "U_a = np.random.randn(16, 32)\n",
    "v_a = np.random.randn(1, 16)\n",
    "\n",
    "print(\"注意力評分函數：\")\n",
    "print(f\"  Bahdanau（加性）：score = v^T tanh(W*s + U*h)\")\n",
    "print(f\"  點積：score = s^T h\")\n",
    "print(f\"  縮放點積：score = s^T h / sqrt(d_k)\")\n",
    "print(f\"\\nBahdanau 更具表達力但有更多參數。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "### 注意力解決的問題：\n",
    "- **固定長度上下文**：整個輸入壓縮成單一向量\n",
    "- **資訊瓶頸**：長序列會丟失資訊\n",
    "- **無對齊**：解碼器不知道該關注哪個輸入\n",
    "\n",
    "### Bahdanau 注意力的創新：\n",
    "1. **動態上下文**：每個解碼器步驟不同\n",
    "2. **軟對齊**：學習對齊源和目標\n",
    "3. **所有編碼器狀態**：解碼器可存取所有狀態，而不只是最後一個\n",
    "\n",
    "### 運作方式：\n",
    "```\n",
    "1. 編碼器產生標註 h_1, ..., h_T\n",
    "2. 對於每個解碼器步驟 i：\n",
    "   a. 計算注意力分數：e_ij = score(s_{i-1}, h_j)\n",
    "   b. 正規化為權重：α_ij = softmax(e_ij)\n",
    "   c. 計算上下文：c_i = Σ α_ij * h_j\n",
    "   d. 生成輸出：y_i = f(s_i, c_i, y_{i-1})\n",
    "```\n",
    "\n",
    "### Bahdanau vs Luong 注意力：\n",
    "| 特性 | Bahdanau (2014) | Luong (2015) |\n",
    "|------|----------------|---------------|\n",
    "| 分數 | 加性：v·tanh(W·s + U·h) | 乘性：s·h |\n",
    "| 時機 | 使用 s_{i-1}（前一個） | 使用 s_i（當前） |\n",
    "| 全域/局部 | 僅全域 | 兩者都有 |\n",
    "\n",
    "### 數學公式：\n",
    "\n",
    "**注意力分數（對齊模型）**：\n",
    "$$e_{ij} = v_a^T \\tanh(W_a s_{i-1} + U_a h_j)$$\n",
    "\n",
    "**注意力權重**：\n",
    "$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}$$\n",
    "\n",
    "**上下文向量**：\n",
    "$$c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j$$\n",
    "\n",
    "**解碼器**：\n",
    "$$s_i = f(s_{i-1}, y_{i-1}, c_i)$$\n",
    "$$p(y_i | y_{<i}, x) = g(s_i, y_{i-1}, c_i)$$\n",
    "\n",
    "### 影響：\n",
    "- **革新 NMT**：BLEU 分數大幅躍升\n",
    "- **可解釋性**：可以視覺化對齊\n",
    "- **Transformers 的基礎**：純注意力（2017）\n",
    "- **超越 NMT**：用於視覺、語音等\n",
    "\n",
    "### 為什麼有效：\n",
    "1. **解決瓶頸**：可變長度上下文\n",
    "2. **學習對齊**：不需要獨立的對齊模型\n",
    "3. **可微分**：端到端訓練\n",
    "4. **適用長序列**：注意力不會衰減\n",
    "\n",
    "### 現代觀點：\n",
    "- Transformers 使用**自注意力**（注意同一序列）\n",
    "- 縮放點積現在是標準（更簡單、更快）\n",
    "- 多頭注意力捕捉不同的關係\n",
    "- 但 Bahdanau 的核心思想依然存在：**注意相關的東西**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
