{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 4：遞迴神經網路正則化（Recurrent Neural Network Regularization）\n",
    "## Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals (2014)\n",
    "\n",
    "### RNN 的 Dropout\n",
    "\n",
    "關鍵洞察：**僅對非遞迴連接應用 dropout**，不對遞迴連接使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, dropout_rate=0.5, training=True):\n",
    "    \"\"\"\n",
    "    標準 dropout\n",
    "    訓練時：以 dropout_rate 的機率隨機將元素歸零\n",
    "    測試時：乘以 (1 - dropout_rate) 進行縮放\n",
    "    \"\"\"\n",
    "    if not training or dropout_rate == 0:\n",
    "        return x\n",
    "    \n",
    "    # 反向 dropout（訓練時縮放）\n",
    "    mask = (np.random.rand(*x.shape) > dropout_rate).astype(float)\n",
    "    return x * mask / (1 - dropout_rate)\n",
    "\n",
    "# 測試 dropout\n",
    "x = np.ones((5, 1))\n",
    "print(\"原始值：\", x.T)\n",
    "print(\"加入 dropout (p=0.5)：\", dropout(x, 0.5).T)\n",
    "print(\"加入 dropout (p=0.5)：\", dropout(x, 0.5).T)\n",
    "print(\"測試模式：\", dropout(x, 0.5, training=False).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具有正確 Dropout 的 RNN\n",
    "\n",
    "**關鍵**：在**輸入**和**輸出**上使用 dropout，**不**在遞迴連接上使用！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithDropout:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 權重\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs, dropout_rate=0.0, training=True):\n",
    "        \"\"\"\n",
    "        帶有 dropout 的前向傳遞\n",
    "        \n",
    "        Dropout 應用於：\n",
    "        1. 輸入連接 (x -> h)\n",
    "        2. 輸出連接 (h -> y)\n",
    "        \n",
    "        不應用於：\n",
    "        - 遞迴連接 (h -> h)\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        for x in inputs:\n",
    "            # 對輸入應用 dropout\n",
    "            x_dropped = dropout(x, dropout_rate, training)\n",
    "            \n",
    "            # RNN 更新（遞迴連接不使用 dropout）\n",
    "            h = np.tanh(\n",
    "                np.dot(self.W_xh, x_dropped) +  # 這裡有 Dropout\n",
    "                np.dot(self.W_hh, h) +           # 這裡沒有 Dropout\n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # 在輸出前對隱藏狀態應用 dropout\n",
    "            h_dropped = dropout(h, dropout_rate, training)\n",
    "            \n",
    "            # 輸出\n",
    "            y = np.dot(self.W_hy, h_dropped) + self.by  # 這裡有 Dropout\n",
    "            \n",
    "            outputs.append(y)\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "\n",
    "# 測試\n",
    "rnn = RNNWithDropout(input_size=10, hidden_size=20, output_size=10)\n",
    "test_inputs = [np.random.randn(10, 1) for _ in range(5)]\n",
    "\n",
    "outputs_train, _ = rnn.forward(test_inputs, dropout_rate=0.5, training=True)\n",
    "outputs_test, _ = rnn.forward(test_inputs, dropout_rate=0.5, training=False)\n",
    "\n",
    "print(f\"訓練輸出[0] 平均值：{outputs_train[0].mean():.4f}\")\n",
    "print(f\"測試輸出[0] 平均值：{outputs_test[0].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 變分 Dropout（Variational Dropout）\n",
    "\n",
    "**關鍵創新**：在所有時間步使用**相同的** dropout 遮罩！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithVariationalDropout:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 權重（與之前相同）\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs, dropout_rate=0.0, training=True):\n",
    "        \"\"\"\n",
    "        變分 dropout：所有時間步使用相同遮罩\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        # 為整個序列生成一次遮罩\n",
    "        if training and dropout_rate > 0:\n",
    "            input_mask = (np.random.rand(self.input_size, 1) > dropout_rate).astype(float) / (1 - dropout_rate)\n",
    "            hidden_mask = (np.random.rand(self.hidden_size, 1) > dropout_rate).astype(float) / (1 - dropout_rate)\n",
    "        else:\n",
    "            input_mask = np.ones((self.input_size, 1))\n",
    "            hidden_mask = np.ones((self.hidden_size, 1))\n",
    "        \n",
    "        for x in inputs:\n",
    "            # 對每個輸入應用相同遮罩\n",
    "            x_dropped = x * input_mask\n",
    "            \n",
    "            # RNN 更新\n",
    "            h = np.tanh(\n",
    "                np.dot(self.W_xh, x_dropped) +\n",
    "                np.dot(self.W_hh, h) +\n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # 對每個隱藏狀態應用相同遮罩\n",
    "            h_dropped = h * hidden_mask\n",
    "            \n",
    "            # 輸出\n",
    "            y = np.dot(self.W_hy, h_dropped) + self.by\n",
    "            \n",
    "            outputs.append(y)\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "\n",
    "# 測試變分 dropout\n",
    "var_rnn = RNNWithVariationalDropout(input_size=10, hidden_size=20, output_size=10)\n",
    "outputs_var, _ = var_rnn.forward(test_inputs, dropout_rate=0.5, training=True)\n",
    "\n",
    "print(\"變分 dropout 在所有時間步使用一致的遮罩\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比較 Dropout 策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成合成序列資料\n",
    "seq_length = 20\n",
    "test_sequence = [np.random.randn(10, 1) for _ in range(seq_length)]\n",
    "\n",
    "# 使用不同策略運行\n",
    "_, h_no_dropout = rnn.forward(test_sequence, dropout_rate=0.0, training=False)\n",
    "_, h_standard = rnn.forward(test_sequence, dropout_rate=0.5, training=True)\n",
    "_, h_variational = var_rnn.forward(test_sequence, dropout_rate=0.5, training=True)\n",
    "\n",
    "# 轉換為陣列\n",
    "h_no_dropout = np.hstack([h.flatten() for h in h_no_dropout]).T\n",
    "h_standard = np.hstack([h.flatten() for h in h_standard]).T\n",
    "h_variational = np.hstack([h.flatten() for h in h_variational]).T\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].imshow(h_no_dropout, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title('無 Dropout')\n",
    "axes[0].set_xlabel('隱藏單元')\n",
    "axes[0].set_ylabel('時間步')\n",
    "\n",
    "axes[1].imshow(h_standard, cmap='RdBu', aspect='auto')\n",
    "axes[1].set_title('標準 Dropout（每個時間步不同遮罩）')\n",
    "axes[1].set_xlabel('隱藏單元')\n",
    "axes[1].set_ylabel('時間步')\n",
    "\n",
    "axes[2].imshow(h_variational, cmap='RdBu', aspect='auto')\n",
    "axes[2].set_title('變分 Dropout（所有時間步相同遮罩）')\n",
    "axes[2].set_xlabel('隱藏單元')\n",
    "axes[2].set_ylabel('時間步')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"變分 dropout 顯示一致的模式（相同單元在整個過程中被丟棄）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout 位置很重要！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 dropout 應用位置\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 建立簡單的 RNN 圖示\n",
    "def draw_rnn_cell(ax, title, show_input_dropout, show_hidden_dropout, show_recurrent_dropout):\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 繪製方塊\n",
    "    # 輸入\n",
    "    ax.add_patch(plt.Rectangle((1, 2), 1.5, 1, fill=True, color='lightblue', ec='black'))\n",
    "    ax.text(1.75, 2.5, 'x_t', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # 隱藏狀態（當前）\n",
    "    ax.add_patch(plt.Rectangle((4, 4.5), 2, 2, fill=True, color='lightgreen', ec='black'))\n",
    "    ax.text(5, 5.5, 'h_t', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # 隱藏狀態（前一個）\n",
    "    ax.add_patch(plt.Rectangle((7, 4.5), 2, 2, fill=True, color='lightyellow', ec='black'))\n",
    "    ax.text(8, 5.5, 'h_{t-1}', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # 輸出\n",
    "    ax.add_patch(plt.Rectangle((4, 7.5), 2, 1, fill=True, color='lightcoral', ec='black'))\n",
    "    ax.text(5, 8, 'y_t', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # 箭頭\n",
    "    # 輸入到隱藏\n",
    "    color_input = 'red' if show_input_dropout else 'black'\n",
    "    width_input = 3 if show_input_dropout else 1\n",
    "    ax.arrow(2.5, 2.5, 1.3, 2, head_width=0.3, color=color_input, lw=width_input)\n",
    "    if show_input_dropout:\n",
    "        ax.text(3.2, 3.5, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "    \n",
    "    # 遞迴\n",
    "    color_rec = 'red' if show_recurrent_dropout else 'black'\n",
    "    width_rec = 3 if show_recurrent_dropout else 1\n",
    "    ax.arrow(7, 5.5, -0.8, 0, head_width=0.3, color=color_rec, lw=width_rec)\n",
    "    if show_recurrent_dropout:\n",
    "        ax.text(6.5, 6.2, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "    \n",
    "    # 隱藏到輸出\n",
    "    color_hidden = 'red' if show_hidden_dropout else 'black'\n",
    "    width_hidden = 3 if show_hidden_dropout else 1\n",
    "    ax.arrow(5, 6.6, 0, 0.7, head_width=0.3, color=color_hidden, lw=width_hidden)\n",
    "    if show_hidden_dropout:\n",
    "        ax.text(5.5, 7, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "\n",
    "# 錯誤：到處都用 dropout\n",
    "draw_rnn_cell(axes[0, 0], '錯誤：到處都用 Dropout\\n（破壞時序流動）', \n",
    "             show_input_dropout=True, show_hidden_dropout=True, show_recurrent_dropout=True)\n",
    "\n",
    "# 錯誤：只在遞迴連接\n",
    "draw_rnn_cell(axes[0, 1], '錯誤：只在遞迴連接\\n（失去梯度流動）', \n",
    "             show_input_dropout=False, show_hidden_dropout=False, show_recurrent_dropout=True)\n",
    "\n",
    "# 正確：Zaremba 等人的方法\n",
    "draw_rnn_cell(axes[1, 0], '正確：Zaremba 等人\\n（僅輸入和輸出）', \n",
    "             show_input_dropout=True, show_hidden_dropout=True, show_recurrent_dropout=False)\n",
    "\n",
    "# 無 dropout\n",
    "draw_rnn_cell(axes[1, 1], '基準：無 Dropout\\n（可能過擬合）', \n",
    "             show_input_dropout=False, show_hidden_dropout=False, show_recurrent_dropout=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "### 問題：\n",
    "- 在 RNN 上使用樸素 dropout 效果不好\n",
    "- 丟棄遞迴連接會破壞時序資訊流動\n",
    "- 標準 dropout 每個時間步都改變遮罩（有雜訊）\n",
    "\n",
    "### Zaremba 等人的解決方案：\n",
    "\n",
    "**應用 dropout 於：**\n",
    "- ✅ 輸入到隱藏的連接 (W_xh)\n",
    "- ✅ 隱藏到輸出的連接 (W_hy)\n",
    "\n",
    "**不應用於：**\n",
    "- ❌ 遞迴連接 (W_hh)\n",
    "\n",
    "### 變分 Dropout：\n",
    "- 所有時間步使用**相同的 dropout 遮罩**\n",
    "- 比改變遮罩更穩定\n",
    "- 有更好的理論依據（貝葉斯）\n",
    "\n",
    "### 結果：\n",
    "- 語言建模有顯著改進\n",
    "- Penn Treebank：測試困惑度從 78.4 改善到 68.7\n",
    "- 也適用於 LSTM 和 GRU\n",
    "\n",
    "### 實作技巧：\n",
    "1. 使用比前饋網路更高的 dropout 率 (0.5-0.7)\n",
    "2. 對雙向 RNN 在**兩個**方向都應用 dropout\n",
    "3. 可以堆疊多個 LSTM 層，在層間使用 dropout\n",
    "4. 變分 dropout：每個序列只生成一次遮罩\n",
    "\n",
    "### 為什麼有效：\n",
    "- 保持時序依賴（遞迴不使用 dropout）\n",
    "- 正則化非時序轉換\n",
    "- 強制對缺失輸入特徵的魯棒性\n",
    "- 一致的遮罩（變分）減少變異"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
