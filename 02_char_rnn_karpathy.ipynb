{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 2: The Unreasonable Effectiveness of Recurrent Neural Networks\n",
    "## Andrej Karpathy\n",
    "\n",
    "### Character-Level Language Model with Vanilla RNN\n",
    "\n",
    "Implementation of a character-level RNN that learns to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple synthetic text with patterns\n",
    "data = \"\"\"\n",
    "hello world\n",
    "hello deep learning\n",
    "deep neural networks\n",
    "neural networks learn patterns\n",
    "patterns in data\n",
    "data drives learning\n",
    "learning from examples\n",
    "examples help networks\n",
    "networks process information\n",
    "information is everywhere\n",
    "everywhere you look data\n",
    "\"\"\" * 10  # Repeat for more training data\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Data length: {len(data)} characters\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {repr(''.join(chars))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "        \"\"\"\n",
    "        inputs: list of integers (character indices)\n",
    "        hprev: initial hidden state\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        loss = 0\n",
    "        \n",
    "        # Forward pass\n",
    "        for t, char_idx in enumerate(inputs):\n",
    "            # One-hot encode input\n",
    "            xs[t] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t][char_idx] = 1\n",
    "            \n",
    "            # Hidden state: h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)\n",
    "            hs[t] = np.tanh(\n",
    "                np.dot(self.Wxh, xs[t]) + \n",
    "                np.dot(self.Whh, hs[t-1]) + \n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # Output: y_t = W_hy * h_t + b_y\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            \n",
    "            # Softmax probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "            \n",
    "        return xs, hs, ys, ps\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        loss = 0\n",
    "        for t, target_idx in enumerate(targets):\n",
    "            loss += -np.log(ps[t][target_idx, 0])\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        \"\"\"Backpropagation through time\"\"\"\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        \n",
    "        # Backward pass\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # Output gradient\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            \n",
    "            # Output layer gradients\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            \n",
    "            # Hidden layer gradient\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhraw = (1 - hs[t] ** 2) * dh  # tanh derivative\n",
    "            \n",
    "            # Weight gradients\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            \n",
    "            # Gradient for next timestep\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "    \n",
    "    def sample(self, h, seed_ix, n):\n",
    "        \"\"\"\n",
    "        Sample a sequence of characters from the model\n",
    "        h: initial hidden state\n",
    "        seed_ix: seed character index\n",
    "        n: number of characters to generate\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        indices = []\n",
    "        \n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            \n",
    "            # Sample from distribution\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            \n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            indices.append(ix)\n",
    "        \n",
    "        return indices\n",
    "\n",
    "# Initialize model\n",
    "hidden_size = 64\n",
    "rnn = VanillaRNN(vocab_size, hidden_size)\n",
    "print(f\"\\nModel initialized with {hidden_size} hidden units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, data, char_to_ix, ix_to_char, num_iterations=2000, seq_length=25):\n",
    "    \"\"\"Train the RNN\"\"\"\n",
    "    n = 0  # Position in data\n",
    "    p = 0  # Data pointer\n",
    "    \n",
    "    # Memory variables for Adagrad\n",
    "    mWxh = np.zeros_like(rnn.Wxh)\n",
    "    mWhh = np.zeros_like(rnn.Whh)\n",
    "    mWhy = np.zeros_like(rnn.Why)\n",
    "    mbh = np.zeros_like(rnn.bh)\n",
    "    mby = np.zeros_like(rnn.by)\n",
    "    \n",
    "    smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "    losses = []\n",
    "    \n",
    "    hprev = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    for n in range(num_iterations):\n",
    "        # Prepare inputs and targets\n",
    "        if p + seq_length + 1 >= len(data) or n == 0:\n",
    "            hprev = np.zeros((hidden_size, 1))\n",
    "            p = 0\n",
    "        \n",
    "        inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "        \n",
    "        # Forward pass\n",
    "        xs, hs, ys, ps = rnn.forward(inputs, hprev)\n",
    "        loss = rnn.loss(ps, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        dWxh, dWhh, dWhy, dbh, dby = rnn.backward(xs, hs, ps, targets)\n",
    "        \n",
    "        # Adagrad parameter update\n",
    "        learning_rate = 0.1\n",
    "        for param, dparam, mem in zip(\n",
    "            [rnn.Wxh, rnn.Whh, rnn.Why, rnn.bh, rnn.by],\n",
    "            [dWxh, dWhh, dWhy, dbh, dby],\n",
    "            [mWxh, mWhh, mWhy, mbh, mby]\n",
    "        ):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "        \n",
    "        # Track loss\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "        losses.append(smooth_loss)\n",
    "        \n",
    "        # Sample from model\n",
    "        if n % 200 == 0:\n",
    "            sample_ix = rnn.sample(hprev, inputs[0], 100)\n",
    "            txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "            print(f\"\\n--- Iteration {n}, Loss: {smooth_loss:.4f} ---\")\n",
    "            print(txt)\n",
    "        \n",
    "        # Move data pointer\n",
    "        p += seq_length\n",
    "        hprev = hs[len(inputs) - 1]\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train the model\n",
    "print(\"Training RNN...\\n\")\n",
    "losses = train_rnn(rnn, data, char_to_ix, ix_to_char, num_iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Smooth Loss')\n",
    "plt.title('RNN Training Loss (Character-Level Language Model)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text from Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with different seeds\n",
    "h = np.zeros((hidden_size, 1))\n",
    "\n",
    "print(\"Generated samples:\\n\")\n",
    "for i in range(5):\n",
    "    seed_char = np.random.choice(chars)\n",
    "    seed_ix = char_to_ix[seed_char]\n",
    "    sample_ix = rnn.sample(h, seed_ix, 150)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print(f\"Sample {i+1} (seed: '{seed_char}'):\")\n",
    "    print(txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Hidden State Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through a sequence to visualize activations\n",
    "test_text = \"hello deep learning\"\n",
    "test_inputs = [char_to_ix[ch] for ch in test_text]\n",
    "hprev = np.zeros((hidden_size, 1))\n",
    "\n",
    "xs, hs, ys, ps = rnn.forward(test_inputs, hprev)\n",
    "\n",
    "# Extract hidden states\n",
    "hidden_states = np.array([hs[t].flatten() for t in range(len(test_inputs))])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(hidden_states.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Activation')\n",
    "plt.xlabel('Time Step (Character Position)')\n",
    "plt.ylabel('Hidden Unit')\n",
    "plt.title('RNN Hidden State Activations')\n",
    "plt.xticks(range(len(test_text)), list(test_text))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization shows how hidden states evolve as RNN processes '{test_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Character-Level Modeling**: RNNs can learn to generate text character-by-character\n",
    "2. **Recurrent Connections**: Hidden state carries information across time steps\n",
    "3. **Backpropagation Through Time**: Gradients flow backwards through sequences\n",
    "4. **Gradient Clipping**: Essential to prevent exploding gradients\n",
    "5. **Sampling**: Temperature control in sampling affects diversity\n",
    "\n",
    "### The Unreasonable Effectiveness:\n",
    "- Simple RNN architecture can learn complex patterns\n",
    "- No explicit feature engineering needed\n",
    "- Learns hierarchical representations automatically\n",
    "- Generalizes to unseen character combinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
