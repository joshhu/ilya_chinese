{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 2：遞迴神經網路的不合理有效性（The Unreasonable Effectiveness of RNNs）\n",
    "## Andrej Karpathy\n",
    "\n",
    "### 使用原始 RNN 的字元級語言模型\n",
    "\n",
    "實作一個學習生成文本的字元級 RNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成合成訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 帶有模式的簡單合成文本\n",
    "data = \"\"\"\n",
    "hello world\n",
    "hello deep learning\n",
    "deep neural networks\n",
    "neural networks learn patterns\n",
    "patterns in data\n",
    "data drives learning\n",
    "learning from examples\n",
    "examples help networks\n",
    "networks process information\n",
    "information is everywhere\n",
    "everywhere you look data\n",
    "\"\"\" * 10  # 重複以獲得更多訓練資料\n",
    "\n",
    "# 建立詞彙表\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"資料長度：{len(data)} 個字元\")\n",
    "print(f\"詞彙表大小：{vocab_size}\")\n",
    "print(f\"詞彙表：{repr(''.join(chars))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始 RNN 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 初始化權重\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "        \"\"\"\n",
    "        前向傳遞\n",
    "        inputs：整數列表（字元索引）\n",
    "        hprev：初始隱藏狀態\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        loss = 0\n",
    "        \n",
    "        # 前向傳遞\n",
    "        for t, char_idx in enumerate(inputs):\n",
    "            # 輸入的 one-hot 編碼\n",
    "            xs[t] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t][char_idx] = 1\n",
    "            \n",
    "            # 隱藏狀態：h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)\n",
    "            hs[t] = np.tanh(\n",
    "                np.dot(self.Wxh, xs[t]) + \n",
    "                np.dot(self.Whh, hs[t-1]) + \n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # 輸出：y_t = W_hy * h_t + b_y\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            \n",
    "            # Softmax 機率\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "            \n",
    "        return xs, hs, ys, ps\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"交叉熵損失\"\"\"\n",
    "        loss = 0\n",
    "        for t, target_idx in enumerate(targets):\n",
    "            loss += -np.log(ps[t][target_idx, 0])\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        \"\"\"時間反向傳播（BPTT）\"\"\"\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        \n",
    "        # 反向傳遞\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # 輸出梯度\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            \n",
    "            # 輸出層梯度\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            \n",
    "            # 隱藏層梯度\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhraw = (1 - hs[t] ** 2) * dh  # tanh 導數\n",
    "            \n",
    "            # 權重梯度\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            \n",
    "            # 下一時間步的梯度\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        \n",
    "        # 裁剪梯度以防止梯度爆炸\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "    \n",
    "    def sample(self, h, seed_ix, n):\n",
    "        \"\"\"\n",
    "        從模型中採樣字元序列\n",
    "        h：初始隱藏狀態\n",
    "        seed_ix：種子字元索引\n",
    "        n：要生成的字元數\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        indices = []\n",
    "        \n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            \n",
    "            # 從分佈中採樣\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            \n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            indices.append(ix)\n",
    "        \n",
    "        return indices\n",
    "\n",
    "# 初始化模型\n",
    "hidden_size = 64\n",
    "rnn = VanillaRNN(vocab_size, hidden_size)\n",
    "print(f\"\\n模型已初始化，隱藏單元數：{hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, data, char_to_ix, ix_to_char, num_iterations=2000, seq_length=25):\n",
    "    \"\"\"訓練 RNN\"\"\"\n",
    "    n = 0  # 資料中的位置\n",
    "    p = 0  # 資料指標\n",
    "    \n",
    "    # Adagrad 的記憶變數\n",
    "    mWxh = np.zeros_like(rnn.Wxh)\n",
    "    mWhh = np.zeros_like(rnn.Whh)\n",
    "    mWhy = np.zeros_like(rnn.Why)\n",
    "    mbh = np.zeros_like(rnn.bh)\n",
    "    mby = np.zeros_like(rnn.by)\n",
    "    \n",
    "    smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "    losses = []\n",
    "    \n",
    "    hprev = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    for n in range(num_iterations):\n",
    "        # 準備輸入和目標\n",
    "        if p + seq_length + 1 >= len(data) or n == 0:\n",
    "            hprev = np.zeros((hidden_size, 1))\n",
    "            p = 0\n",
    "        \n",
    "        inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "        \n",
    "        # 前向傳遞\n",
    "        xs, hs, ys, ps = rnn.forward(inputs, hprev)\n",
    "        loss = rnn.loss(ps, targets)\n",
    "        \n",
    "        # 反向傳遞\n",
    "        dWxh, dWhh, dWhy, dbh, dby = rnn.backward(xs, hs, ps, targets)\n",
    "        \n",
    "        # Adagrad 參數更新\n",
    "        learning_rate = 0.1\n",
    "        for param, dparam, mem in zip(\n",
    "            [rnn.Wxh, rnn.Whh, rnn.Why, rnn.bh, rnn.by],\n",
    "            [dWxh, dWhh, dWhy, dbh, dby],\n",
    "            [mWxh, mWhh, mWhy, mbh, mby]\n",
    "        ):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "        \n",
    "        # 追蹤損失\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "        losses.append(smooth_loss)\n",
    "        \n",
    "        # 從模型採樣\n",
    "        if n % 200 == 0:\n",
    "            sample_ix = rnn.sample(hprev, inputs[0], 100)\n",
    "            txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "            print(f\"\\n--- 迭代 {n}，損失：{smooth_loss:.4f} ---\")\n",
    "            print(txt)\n",
    "        \n",
    "        # 移動資料指標\n",
    "        p += seq_length\n",
    "        hprev = hs[len(inputs) - 1]\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 訓練模型\n",
    "print(\"正在訓練 RNN...\\n\")\n",
    "losses = train_rnn(rnn, data, char_to_ix, ix_to_char, num_iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化訓練進度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('迭代次數')\n",
    "plt.ylabel('平滑損失')\n",
    "plt.title('RNN 訓練損失（字元級語言模型）')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 從訓練好的模型生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用不同的種子生成樣本\n",
    "h = np.zeros((hidden_size, 1))\n",
    "\n",
    "print(\"生成的樣本：\\n\")\n",
    "for i in range(5):\n",
    "    seed_char = np.random.choice(chars)\n",
    "    seed_ix = char_to_ix[seed_char]\n",
    "    sample_ix = rnn.sample(h, seed_ix, 150)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print(f\"樣本 {i+1}（種子：'{seed_char}'）：\")\n",
    "    print(txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化隱藏狀態激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通過一個序列的前向傳遞來視覺化激活\n",
    "test_text = \"hello deep learning\"\n",
    "test_inputs = [char_to_ix[ch] for ch in test_text]\n",
    "hprev = np.zeros((hidden_size, 1))\n",
    "\n",
    "xs, hs, ys, ps = rnn.forward(test_inputs, hprev)\n",
    "\n",
    "# 提取隱藏狀態\n",
    "hidden_states = np.array([hs[t].flatten() for t in range(len(test_inputs))])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(hidden_states.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='激活值')\n",
    "plt.xlabel('時間步（字元位置）')\n",
    "plt.ylabel('隱藏單元')\n",
    "plt.title('RNN 隱藏狀態激活')\n",
    "plt.xticks(range(len(test_text)), list(test_text))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n視覺化展示了 RNN 處理 '{test_text}' 時隱藏狀態的演變\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "1. **字元級建模**：RNN 可以學習逐字元生成文本\n",
    "2. **遞迴連接**：隱藏狀態在時間步之間傳遞資訊\n",
    "3. **時間反向傳播（BPTT）**：梯度通過序列向後流動\n",
    "4. **梯度裁剪**：防止梯度爆炸的必要措施\n",
    "5. **採樣**：採樣中的溫度控制影響多樣性\n",
    "\n",
    "### 不合理的有效性：\n",
    "- 簡單的 RNN 架構可以學習複雜的模式\n",
    "- 不需要明確的特徵工程\n",
    "- 自動學習層次化表示\n",
    "- 可以泛化到未見過的字元組合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
