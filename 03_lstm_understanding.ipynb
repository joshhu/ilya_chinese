{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 3：理解 LSTM 網路（Understanding LSTM Networks）\n",
    "## Christopher Olah\n",
    "\n",
    "### 實作 LSTM 及閘門視覺化\n",
    "\n",
    "LSTM（長短期記憶）網路透過閘門式記憶單元解決梯度消失問題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 單元實作\n",
    "\n",
    "LSTM 有三個閘門：\n",
    "1. **遺忘閘門（Forget Gate）**：決定從細胞狀態中遺忘什麼\n",
    "2. **輸入閘門（Input Gate）**：決定添加什麼新資訊\n",
    "3. **輸出閘門（Output Gate）**：決定根據細胞狀態輸出什麼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 為了效率將權重串接：[輸入; 隱藏狀態] -> 閘門\n",
    "        concat_size = input_size + hidden_size\n",
    "        \n",
    "        # 遺忘閘門\n",
    "        self.Wf = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # 輸入閘門\n",
    "        self.Wi = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # 候選細胞狀態\n",
    "        self.Wc = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # 輸出閘門\n",
    "        self.Wo = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        LSTM 單元的前向傳遞\n",
    "        \n",
    "        x: 輸入 (input_size, 1)\n",
    "        h_prev: 前一個隱藏狀態 (hidden_size, 1)\n",
    "        c_prev: 前一個細胞狀態 (hidden_size, 1)\n",
    "        \n",
    "        回傳：\n",
    "        h_next: 下一個隱藏狀態\n",
    "        c_next: 下一個細胞狀態\n",
    "        cache: 反向傳遞需要的值\n",
    "        \"\"\"\n",
    "        # 串接輸入和前一個隱藏狀態\n",
    "        concat = np.vstack([x, h_prev])\n",
    "        \n",
    "        # 遺忘閘門：決定從細胞狀態中遺忘什麼\n",
    "        f = sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "        \n",
    "        # 輸入閘門：決定儲存什麼新資訊\n",
    "        i = sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "        \n",
    "        # 候選細胞狀態：可能要添加的新資訊\n",
    "        c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "        \n",
    "        # 更新細胞狀態：遺忘 + 輸入新資訊\n",
    "        c_next = f * c_prev + i * c_tilde\n",
    "        \n",
    "        # 輸出閘門：決定輸出什麼\n",
    "        o = sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "        \n",
    "        # 隱藏狀態：經過過濾的細胞狀態\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        # 快取以供反向傳遞使用\n",
    "        cache = (x, h_prev, c_prev, concat, f, i, c_tilde, c_next, o, h_next)\n",
    "        \n",
    "        return h_next, c_next, cache\n",
    "\n",
    "# 測試 LSTM 單元\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "lstm_cell = LSTMCell(input_size, hidden_size)\n",
    "\n",
    "x = np.random.randn(input_size, 1)\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "h_next, c_next, cache = lstm_cell.forward(x, h, c)\n",
    "print(f\"LSTM 單元已初始化：input_size={input_size}, hidden_size={hidden_size}\")\n",
    "print(f\"隱藏狀態形狀：{h_next.shape}\")\n",
    "print(f\"細胞狀態形狀：{c_next.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用於序列處理的完整 LSTM 網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "        # 輸出層\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        通過 LSTM 處理序列\n",
    "        inputs: 輸入向量的列表\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # 儲存狀態以供視覺化\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        gate_values = {'f': [], 'i': [], 'o': []}\n",
    "        \n",
    "        for x in inputs:\n",
    "            h, c, cache = self.cell.forward(x, h, c)\n",
    "            h_states.append(h.copy())\n",
    "            c_states.append(c.copy())\n",
    "            \n",
    "            # 從快取中提取閘門值\n",
    "            _, _, _, _, f, i, _, _, o, _ = cache\n",
    "            gate_values['f'].append(f.copy())\n",
    "            gate_values['i'].append(i.copy())\n",
    "            gate_values['o'].append(o.copy())\n",
    "        \n",
    "        # 最終輸出\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "        \n",
    "        return y, h_states, c_states, gate_values\n",
    "\n",
    "# 建立 LSTM 模型\n",
    "input_size = 5\n",
    "hidden_size = 16\n",
    "output_size = 5\n",
    "lstm = LSTM(input_size, hidden_size, output_size)\n",
    "print(f\"\\nLSTM 模型已建立：{input_size} -> {hidden_size} -> {output_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合成序列任務測試：長期依賴\n",
    "\n",
    "任務：記住序列開頭的值，並在結尾輸出它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_term_dependency_data(seq_length=20, num_samples=100):\n",
    "    \"\"\"\n",
    "    生成需要記住第一個元素直到最後的序列\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # 建立序列\n",
    "        sequence = []\n",
    "        \n",
    "        # 第一個元素是重要的（one-hot 編碼）\n",
    "        first_elem = np.random.randint(0, input_size)\n",
    "        first_vec = np.zeros((input_size, 1))\n",
    "        first_vec[first_elem] = 1\n",
    "        sequence.append(first_vec)\n",
    "        \n",
    "        # 其餘是隨機雜訊\n",
    "        for _ in range(seq_length - 1):\n",
    "            noise = np.random.randn(input_size, 1) * 0.1\n",
    "            sequence.append(noise)\n",
    "        \n",
    "        X.append(sequence)\n",
    "        \n",
    "        # 目標：記住第一個元素\n",
    "        target = np.zeros((output_size, 1))\n",
    "        target[first_elem] = 1\n",
    "        y.append(target)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 生成測試資料\n",
    "X_test, y_test = generate_long_term_dependency_data(seq_length=15, num_samples=10)\n",
    "\n",
    "# 測試前向傳遞\n",
    "output, h_states, c_states, gate_values = lstm.forward(X_test[0])\n",
    "\n",
    "print(f\"\\n測試序列長度：{len(X_test[0])}\")\n",
    "print(f\"第一個元素（需記住）：{np.argmax(X_test[0][0])}\")\n",
    "print(f\"預期輸出：{np.argmax(y_test[0])}\")\n",
    "print(f\"模型輸出（未訓練）：{output.flatten()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化 LSTM 閘門\n",
    "\n",
    "理解 LSTM 的關鍵是觀察閘門如何隨時間運作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理序列並視覺化閘門\n",
    "test_seq = X_test[0]\n",
    "output, h_states, c_states, gate_values = lstm.forward(test_seq)\n",
    "\n",
    "# 轉換為陣列以供繪圖\n",
    "forget_gates = np.hstack(gate_values['f'])\n",
    "input_gates = np.hstack(gate_values['i'])\n",
    "output_gates = np.hstack(gate_values['o'])\n",
    "cell_states = np.hstack(c_states)\n",
    "hidden_states = np.hstack(h_states)\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "\n",
    "# 遺忘閘門\n",
    "axes[0].imshow(forget_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[0].set_title('遺忘閘門（1=保留，0=遺忘）')\n",
    "axes[0].set_ylabel('隱藏單元')\n",
    "axes[0].set_xlabel('時間步')\n",
    "\n",
    "# 輸入閘門\n",
    "axes[1].imshow(input_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1].set_title('輸入閘門（1=接受新資訊，0=忽略新資訊）')\n",
    "axes[1].set_ylabel('隱藏單元')\n",
    "axes[1].set_xlabel('時間步')\n",
    "\n",
    "# 輸出閘門\n",
    "axes[2].imshow(output_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[2].set_title('輸出閘門（1=暴露，0=隱藏）')\n",
    "axes[2].set_ylabel('隱藏單元')\n",
    "axes[2].set_xlabel('時間步')\n",
    "\n",
    "# 細胞狀態\n",
    "im3 = axes[3].imshow(cell_states, cmap='RdBu', aspect='auto')\n",
    "axes[3].set_title('細胞狀態（長期記憶）')\n",
    "axes[3].set_ylabel('隱藏單元')\n",
    "axes[3].set_xlabel('時間步')\n",
    "plt.colorbar(im3, ax=axes[3])\n",
    "\n",
    "# 隱藏狀態\n",
    "im4 = axes[4].imshow(hidden_states, cmap='RdBu', aspect='auto')\n",
    "axes[4].set_title('隱藏狀態（輸出到下一層）')\n",
    "axes[4].set_ylabel('隱藏單元')\n",
    "axes[4].set_xlabel('時間步')\n",
    "plt.colorbar(im4, ax=axes[4])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n閘門解釋：\")\n",
    "print(\"- 遺忘閘門控制從細胞狀態中丟棄什麼資訊\")\n",
    "print(\"- 輸入閘門控制向細胞狀態添加什麼新資訊\")\n",
    "print(\"- 輸出閘門控制從細胞狀態輸出什麼\")\n",
    "print(\"- 細胞狀態是長期記憶的高速公路\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比較 LSTM 與原始 RNN 在長序列上的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        concat_size = input_size + hidden_size\n",
    "        self.Wh = np.random.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        concat = np.vstack([x, h_prev])\n",
    "        h_next = np.tanh(np.dot(self.Wh, concat) + self.bh)\n",
    "        return h_next\n",
    "\n",
    "# 建立原始 RNN 作為比較\n",
    "rnn_cell = VanillaRNNCell(input_size, hidden_size)\n",
    "\n",
    "def process_with_vanilla_rnn(inputs):\n",
    "    h = np.zeros((hidden_size, 1))\n",
    "    h_states = []\n",
    "    \n",
    "    for x in inputs:\n",
    "        h = rnn_cell.forward(x, h)\n",
    "        h_states.append(h.copy())\n",
    "    \n",
    "    return h_states\n",
    "\n",
    "# 用兩種方法處理相同序列\n",
    "rnn_h_states = process_with_vanilla_rnn(test_seq)\n",
    "rnn_hidden = np.hstack(rnn_h_states)\n",
    "\n",
    "# 比較隱藏狀態演化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "im1 = ax1.imshow(rnn_hidden, cmap='RdBu', aspect='auto')\n",
    "ax1.set_title('原始 RNN 隱藏狀態')\n",
    "ax1.set_ylabel('隱藏單元')\n",
    "ax1.set_xlabel('時間步')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "im2 = ax2.imshow(hidden_states, cmap='RdBu', aspect='auto')\n",
    "ax2.set_title('LSTM 隱藏狀態')\n",
    "ax2.set_ylabel('隱藏單元')\n",
    "ax2.set_xlabel('時間步')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n關鍵差異：\")\n",
    "print(\"- LSTM 維護與隱藏狀態分開的細胞狀態\")\n",
    "print(\"- 閘門允許選擇性的資訊流動\")\n",
    "print(\"- 更好的時間梯度流動（解決梯度消失問題）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度流動比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬梯度大小\n",
    "def simulate_gradient_flow(seq_length=30):\n",
    "    \"\"\"\n",
    "    模擬原始 RNN 與 LSTM 中梯度如何衰減\n",
    "    \"\"\"\n",
    "    # 原始 RNN：梯度指數衰減\n",
    "    rnn_grads = []\n",
    "    grad = 1.0\n",
    "    decay_factor = 0.85  # 原始 RNN 的典型衰減\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        rnn_grads.append(grad)\n",
    "        grad *= decay_factor\n",
    "    \n",
    "    # LSTM：梯度通過細胞狀態高速公路保持\n",
    "    lstm_grads = []\n",
    "    grad = 1.0\n",
    "    forget_gate_avg = 0.95  # 高遺忘閘門值 = 保持梯度\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        lstm_grads.append(grad)\n",
    "        grad *= forget_gate_avg  # 遺忘閘門控制梯度流動\n",
    "    \n",
    "    return np.array(rnn_grads), np.array(lstm_grads)\n",
    "\n",
    "rnn_grads, lstm_grads = simulate_gradient_flow()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(rnn_grads[::-1], label='原始 RNN', linewidth=2)\n",
    "plt.plot(lstm_grads[::-1], label='LSTM', linewidth=2)\n",
    "plt.xlabel('過去的時間步數')\n",
    "plt.ylabel('梯度大小')\n",
    "plt.title('梯度流動：LSTM vs 原始 RNN')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n30 步後的梯度：\")\n",
    "print(f\"原始 RNN：{rnn_grads[-1]:.6f}（已消失）\")\n",
    "print(f\"LSTM：{lstm_grads[-1]:.6f}（已保持）\")\n",
    "print(f\"\\n這就是為什麼 LSTM 能夠學習長期依賴！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "### LSTM 架構：\n",
    "1. **細胞狀態**：資訊跨時間流動的高速公路\n",
    "2. **遺忘閘門**：控制從記憶中移除什麼\n",
    "3. **輸入閘門**：控制添加什麼新資訊\n",
    "4. **輸出閘門**：控制從記憶中輸出什麼\n",
    "\n",
    "### 為什麼 LSTM 有效：\n",
    "- **恆定誤差傳送帶（Constant Error Carousel）**：細胞狀態提供不間斷的梯度流動\n",
    "- **乘法閘門**：讓網路學習何時記住/遺忘\n",
    "- **加法更新**：細胞狀態通過加法更新 (f*c + i*c_tilde)\n",
    "- **梯度保持**：遺忘閘門接近 1 時保持梯度\n",
    "\n",
    "### 相對於原始 RNN 的優勢：\n",
    "- 解決梯度消失問題\n",
    "- 學習長期依賴（100+ 時間步）\n",
    "- 更穩定的訓練\n",
    "- 在實際序列任務中表現更好"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
