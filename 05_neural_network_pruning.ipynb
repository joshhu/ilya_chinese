{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 5：透過最小化描述長度保持神經網路簡單（Keeping Neural Networks Simple）\n",
    "## Hinton & Van Camp (1993) + 現代剪枝技術\n",
    "\n",
    "### 網路剪枝與壓縮\n",
    "\n",
    "關鍵洞察：移除不必要的權重以獲得更簡單、更具泛化能力的網路。更小 = 更好！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用於分類的簡單神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "class SimpleNN:\n",
    "    \"\"\"簡單的 2 層神經網路\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 初始化權重\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "        \n",
    "        # 追蹤剪枝的遮罩\n",
    "        self.mask1 = np.ones_like(self.W1)\n",
    "        self.mask2 = np.ones_like(self.W2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"前向傳遞\"\"\"\n",
    "        # 應用遮罩（針對被剪枝的權重）\n",
    "        W1_masked = self.W1 * self.mask1\n",
    "        W2_masked = self.W2 * self.mask2\n",
    "        \n",
    "        # 隱藏層\n",
    "        self.h = relu(np.dot(X, W1_masked) + self.b1)\n",
    "        \n",
    "        # 輸出層\n",
    "        logits = np.dot(self.h, W2_masked) + self.b2\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"預測類別標籤\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"計算準確率\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"計算總參數和活躍（未剪枝）參數數量\"\"\"\n",
    "        total = self.W1.size + self.b1.size + self.W2.size + self.b2.size\n",
    "        active = int(np.sum(self.mask1) + self.b1.size + np.sum(self.mask2) + self.b2.size)\n",
    "        return total, active\n",
    "\n",
    "# 測試網路\n",
    "nn = SimpleNN(input_dim=10, hidden_dim=20, output_dim=3)\n",
    "X_test = np.random.randn(5, 10)\n",
    "y_test = nn.forward(X_test)\n",
    "print(f\"網路輸出形狀：{y_test.shape}\")\n",
    "total, active = nn.count_parameters()\n",
    "print(f\"參數：{total} 總計, {active} 活躍\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成合成資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n_samples=1000, n_features=20, n_classes=3):\n",
    "    \"\"\"\n",
    "    生成合成分類資料集\n",
    "    每個類別是一個高斯團\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    samples_per_class = n_samples // n_classes\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        # 此類別的隨機中心\n",
    "        center = np.random.randn(n_features) * 3\n",
    "        \n",
    "        # 在中心周圍生成樣本\n",
    "        X_class = np.random.randn(samples_per_class, n_features) + center\n",
    "        y_class = np.full(samples_per_class, c)\n",
    "        \n",
    "        X.append(X_class)\n",
    "        y.append(y_class)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.concatenate(y)\n",
    "    \n",
    "    # 打亂\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 生成資料\n",
    "X_train, y_train = generate_classification_data(n_samples=1000, n_features=20, n_classes=3)\n",
    "X_test, y_test = generate_classification_data(n_samples=300, n_features=20, n_classes=3)\n",
    "\n",
    "print(f\"訓練集：{X_train.shape}, {y_train.shape}\")\n",
    "print(f\"測試集：{X_test.shape}, {y_test.shape}\")\n",
    "print(f\"類別分佈：{np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練基準網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, X_train, y_train, X_test, y_test, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    簡單訓練迴圈\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 前向傳遞\n",
    "        probs = model.forward(X_train)\n",
    "        \n",
    "        # 交叉熵損失\n",
    "        y_one_hot = np.zeros((len(y_train), model.output_dim))\n",
    "        y_one_hot[np.arange(len(y_train)), y_train] = 1\n",
    "        loss = -np.mean(np.sum(y_one_hot * np.log(probs + 1e-8), axis=1))\n",
    "        \n",
    "        # 反向傳遞（簡化版）\n",
    "        batch_size = len(X_train)\n",
    "        dL_dlogits = (probs - y_one_hot) / batch_size\n",
    "        \n",
    "        # W2, b2 的梯度\n",
    "        dL_dW2 = np.dot(model.h.T, dL_dlogits)\n",
    "        dL_db2 = np.sum(dL_dlogits, axis=0)\n",
    "        \n",
    "        # W1, b1 的梯度\n",
    "        dL_dh = np.dot(dL_dlogits, (model.W2 * model.mask2).T)\n",
    "        dL_dh[model.h <= 0] = 0  # ReLU 導數\n",
    "        dL_dW1 = np.dot(X_train.T, dL_dh)\n",
    "        dL_db1 = np.sum(dL_dh, axis=0)\n",
    "        \n",
    "        # 更新權重（僅在遮罩活躍的位置）\n",
    "        model.W1 -= lr * dL_dW1 * model.mask1\n",
    "        model.b1 -= lr * dL_db1\n",
    "        model.W2 -= lr * dL_dW2 * model.mask2\n",
    "        model.b2 -= lr * dL_db2\n",
    "        \n",
    "        # 追蹤指標\n",
    "        train_losses.append(loss)\n",
    "        test_acc = model.accuracy(X_test, y_test)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"輪次 {epoch+1}/{epochs}, 損失：{loss:.4f}, 測試準確率：{test_acc:.2%}\")\n",
    "    \n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "# 訓練基準模型\n",
    "print(\"訓練基準網路...\\n\")\n",
    "baseline_model = SimpleNN(input_dim=20, hidden_dim=50, output_dim=3)\n",
    "train_losses, test_accs = train_network(baseline_model, X_train, y_train, X_test, y_test, epochs=100)\n",
    "\n",
    "baseline_acc = baseline_model.accuracy(X_test, y_test)\n",
    "total_params, active_params = baseline_model.count_parameters()\n",
    "print(f\"\\n基準：{baseline_acc:.2%} 準確率, {active_params} 參數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基於幅度的剪枝\n",
    "\n",
    "移除絕對值最小的權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_by_magnitude(model, pruning_rate):\n",
    "    \"\"\"\n",
    "    剪枝幅度最小的權重\n",
    "    \n",
    "    pruning_rate: 要移除的權重比例 (0-1)\n",
    "    \"\"\"\n",
    "    # 收集所有權重\n",
    "    all_weights = np.concatenate([model.W1.flatten(), model.W2.flatten()])\n",
    "    all_magnitudes = np.abs(all_weights)\n",
    "    \n",
    "    # 找到閾值\n",
    "    threshold = np.percentile(all_magnitudes, pruning_rate * 100)\n",
    "    \n",
    "    # 建立新遮罩\n",
    "    model.mask1 = (np.abs(model.W1) > threshold).astype(float)\n",
    "    model.mask2 = (np.abs(model.W2) > threshold).astype(float)\n",
    "    \n",
    "    print(f\"剪枝閾值：{threshold:.6f}\")\n",
    "    print(f\"已剪枝 {pruning_rate:.1%} 的權重\")\n",
    "    \n",
    "    total, active = model.count_parameters()\n",
    "    print(f\"剩餘參數：{active}/{total} ({active/total:.1%})\")\n",
    "\n",
    "# 測試剪枝\n",
    "import copy\n",
    "pruned_model = copy.deepcopy(baseline_model)\n",
    "\n",
    "print(\"剪枝前：\")\n",
    "acc_before = pruned_model.accuracy(X_test, y_test)\n",
    "print(f\"準確率：{acc_before:.2%}\\n\")\n",
    "\n",
    "print(\"剪枝 50% 的權重...\")\n",
    "prune_by_magnitude(pruned_model, pruning_rate=0.5)\n",
    "\n",
    "print(\"\\n剪枝後（重新訓練前）：\")\n",
    "acc_after = pruned_model.accuracy(X_test, y_test)\n",
    "print(f\"準確率：{acc_after:.2%}\")\n",
    "print(f\"準確率下降：{(acc_before - acc_after):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剪枝後的微調\n",
    "\n",
    "重新訓練剩餘權重以恢復準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"微調剪枝後的網路...\\n\")\n",
    "finetune_losses, finetune_accs = train_network(\n",
    "    pruned_model, X_train, y_train, X_test, y_test, epochs=50, lr=0.005\n",
    ")\n",
    "\n",
    "acc_finetuned = pruned_model.accuracy(X_test, y_test)\n",
    "total, active = pruned_model.count_parameters()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"結果：\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"基準：      {baseline_acc:.2%} 準確率, {total_params} 參數\")\n",
    "print(f\"剪枝 50%：  {acc_finetuned:.2%} 準確率, {active} 參數\")\n",
    "print(f\"壓縮：      {total_params/active:.1f}x 更小\")\n",
    "print(f\"準確率變化：{(acc_finetuned - baseline_acc):+.2%}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 漸進式剪枝\n",
    "\n",
    "逐步增加剪枝率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_pruning(model, X_train, y_train, X_test, y_test, \n",
    "                     target_sparsity=0.9, num_iterations=5):\n",
    "    \"\"\"\n",
    "    漸進式剪枝和微調\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # 初始狀態\n",
    "    total, active = model.count_parameters()\n",
    "    acc = model.accuracy(X_test, y_test)\n",
    "    results.append({\n",
    "        'iteration': 0,\n",
    "        'sparsity': 0.0,\n",
    "        'active_params': active,\n",
    "        'accuracy': acc\n",
    "    })\n",
    "    \n",
    "    # 逐步增加稀疏度\n",
    "    for i in range(num_iterations):\n",
    "        # 此輪的稀疏度\n",
    "        current_sparsity = target_sparsity * (i + 1) / num_iterations\n",
    "        \n",
    "        print(f\"\\n輪次 {i+1}/{num_iterations}：目標稀疏度 {current_sparsity:.1%}\")\n",
    "        \n",
    "        # 剪枝\n",
    "        prune_by_magnitude(model, pruning_rate=current_sparsity)\n",
    "        \n",
    "        # 微調\n",
    "        train_network(model, X_train, y_train, X_test, y_test, epochs=30, lr=0.005)\n",
    "        \n",
    "        # 記錄結果\n",
    "        total, active = model.count_parameters()\n",
    "        acc = model.accuracy(X_test, y_test)\n",
    "        results.append({\n",
    "            'iteration': i + 1,\n",
    "            'sparsity': current_sparsity,\n",
    "            'active_params': active,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 執行漸進式剪枝\n",
    "iterative_model = copy.deepcopy(baseline_model)\n",
    "results = iterative_pruning(iterative_model, X_train, y_train, X_test, y_test, \n",
    "                           target_sparsity=0.95, num_iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化剪枝結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取資料\n",
    "sparsities = [r['sparsity'] for r in results]\n",
    "accuracies = [r['accuracy'] for r in results]\n",
    "active_params = [r['active_params'] for r in results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 準確率 vs 稀疏度\n",
    "ax1.plot(sparsities, accuracies, 'o-', linewidth=2, markersize=10, color='steelblue')\n",
    "ax1.axhline(y=baseline_acc, color='red', linestyle='--', linewidth=2, label='基準')\n",
    "ax1.set_xlabel('稀疏度（剪枝比例）', fontsize=12)\n",
    "ax1.set_ylabel('測試準確率', fontsize=12)\n",
    "ax1.set_title('準確率 vs 稀疏度', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# 參數 vs 準確率\n",
    "ax2.plot(active_params, accuracies, 's-', linewidth=2, markersize=10, color='darkgreen')\n",
    "ax2.axhline(y=baseline_acc, color='red', linestyle='--', linewidth=2, label='基準')\n",
    "ax2.set_xlabel('活躍參數數', fontsize=12)\n",
    "ax2.set_ylabel('測試準確率', fontsize=12)\n",
    "ax2.set_title('準確率 vs 模型大小', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.invert_xaxis()  # 參數較少在右邊\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n關鍵觀察：可以移除 90%+ 的權重，準確率損失極小！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化權重分佈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 基準權重\n",
    "axes[0, 0].hist(baseline_model.W1.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('基準 W1 分佈', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('權重值')\n",
    "axes[0, 0].set_ylabel('頻率')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(baseline_model.W2.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('基準 W2 分佈', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('權重值')\n",
    "axes[0, 1].set_ylabel('頻率')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 剪枝後的權重（僅活躍的）\n",
    "pruned_W1 = iterative_model.W1[iterative_model.mask1 > 0]\n",
    "pruned_W2 = iterative_model.W2[iterative_model.mask2 > 0]\n",
    "\n",
    "axes[1, 0].hist(pruned_W1.flatten(), bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('剪枝後 W1 分佈（僅活躍權重）', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('權重值')\n",
    "axes[1, 0].set_ylabel('頻率')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(pruned_W2.flatten(), bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('剪枝後 W2 分佈（僅活躍權重）', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('權重值')\n",
    "axes[1, 1].set_ylabel('頻率')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"剪枝後的權重具有較大的幅度（小權重已被移除）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化稀疏模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# W1 稀疏模式\n",
    "im1 = ax1.imshow(iterative_model.mask1.T, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax1.set_xlabel('輸入維度', fontsize=12)\n",
    "ax1.set_ylabel('隱藏維度', fontsize=12)\n",
    "ax1.set_title('W1 稀疏模式（綠色=活躍，紅色=剪枝）', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# W2 稀疏模式\n",
    "im2 = ax2.imshow(iterative_model.mask2.T, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax2.set_xlabel('隱藏維度', fontsize=12)\n",
    "ax2.set_ylabel('輸出維度', fontsize=12)\n",
    "ax2.set_title('W2 稀疏模式（綠色=活躍，紅色=剪枝）', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total, active = iterative_model.count_parameters()\n",
    "print(f\"\\n最終稀疏度：{(total - active) / total:.1%}\")\n",
    "print(f\"壓縮比：{total / active:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDL 原則\n",
    "\n",
    "最小描述長度（Minimum Description Length）：更簡單的模型泛化更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mdl(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    簡化的 MDL 計算\n",
    "    \n",
    "    MDL = 模型成本 + 資料成本\n",
    "    - 模型成本：編碼權重所需的位元\n",
    "    - 資料成本：編碼誤差所需的位元\n",
    "    \"\"\"\n",
    "    # 模型成本：參數數量（簡化版）\n",
    "    total, active = model.count_parameters()\n",
    "    model_cost = active  # 每個參數 = 1 個「位元」（簡化）\n",
    "    \n",
    "    # 資料成本：交叉熵損失\n",
    "    probs = model.forward(X_train)\n",
    "    y_one_hot = np.zeros((len(y_train), model.output_dim))\n",
    "    y_one_hot[np.arange(len(y_train)), y_train] = 1\n",
    "    data_cost = -np.sum(y_one_hot * np.log(probs + 1e-8))\n",
    "    \n",
    "    total_cost = model_cost + data_cost\n",
    "    \n",
    "    return {\n",
    "        'model_cost': model_cost,\n",
    "        'data_cost': data_cost,\n",
    "        'total_cost': total_cost\n",
    "    }\n",
    "\n",
    "# 比較不同模型的 MDL\n",
    "baseline_mdl = compute_mdl(baseline_model, X_train, y_train)\n",
    "pruned_mdl = compute_mdl(iterative_model, X_train, y_train)\n",
    "\n",
    "print(\"MDL 比較：\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'模型':<20} {'模型成本':<15} {'資料成本':<15} {'總計'}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'基準':<20} {baseline_mdl['model_cost']:<15.0f} {baseline_mdl['data_cost']:<15.2f} {baseline_mdl['total_cost']:.2f}\")\n",
    "print(f\"{'剪枝 (95%)':<20} {pruned_mdl['model_cost']:<15.0f} {pruned_mdl['data_cost']:<15.2f} {pruned_mdl['total_cost']:.2f}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n剪枝後的模型總成本更低 → 更好的泛化能力！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "### 神經網路剪枝：\n",
    "\n",
    "**核心理念**：移除不必要的權重以建立更簡單、更小的網路\n",
    "\n",
    "### 基於幅度的剪枝：\n",
    "\n",
    "1. **訓練**網路至正常水準\n",
    "2. **識別**低幅度權重：$|w| < \\text{閾值}$\n",
    "3. **移除**這些權重（設為 0，遮罩掉）\n",
    "4. **微調**剩餘權重\n",
    "\n",
    "### 漸進式剪枝：\n",
    "\n",
    "比一次性剪枝更好：\n",
    "```\n",
    "for 輪次 in 1..N:\n",
    "    剪枝一小部分（例如 20%）\n",
    "    微調\n",
    "```\n",
    "\n",
    "讓網路逐漸適應。\n",
    "\n",
    "### 典型結果：\n",
    "\n",
    "- **50% 稀疏度**：通常沒有準確率損失\n",
    "- **90% 稀疏度**：輕微準確率損失（<2%）\n",
    "- **95%+ 稀疏度**：明顯下降\n",
    "\n",
    "現代網路（ResNets、Transformers）通常可以剪枝到 **90-95% 稀疏度**而影響極小！\n",
    "\n",
    "### MDL 原則：\n",
    "\n",
    "$$\n",
    "\\text{MDL} = \\underbrace{L(\\text{模型})}_{\\text{複雜度}} + \\underbrace{L(\\text{資料 | 模型})}_{\\text{誤差}}\n",
    "$$\n",
    "\n",
    "**奧卡姆剃刀**：能擬合資料的最簡單解釋（最小網路）是最好的。\n",
    "\n",
    "### 剪枝的好處：\n",
    "\n",
    "1. **更小的模型**：更少記憶體，更快推論\n",
    "2. **更好的泛化**：移除過擬合參數\n",
    "3. **能源效率**：更少運算\n",
    "4. **可解釋性**：更簡單的結構\n",
    "\n",
    "### 剪枝類型：\n",
    "\n",
    "| 類型 | 移除什麼 | 加速 |\n",
    "|------|---------|------|\n",
    "| **非結構化** | 個別權重 | 低（稀疏運算） |\n",
    "| **結構化** | 整個神經元/濾波器 | 高（密集運算） |\n",
    "| **通道** | 整個通道 | 高 |\n",
    "| **層** | 整層 | 非常高 |\n",
    "\n",
    "### 關鍵洞察：\n",
    "\n",
    "**神經網路嚴重過度參數化！**\n",
    "\n",
    "大多數權重對最終效能貢獻很小。剪枝揭示了做實際工作的「核心」網路。\n",
    "\n",
    "**「最好的模型是能擬合資料的最簡單模型」** - MDL 原則"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
