{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 9：GPipe - 使用管線平行化高效訓練巨型神經網路\n",
    "\n",
    "**論文**：Huang et al. (2019) - GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism\n",
    "\n",
    "**核心洞見**：訓練非常大的神經網路需要將它們分割到多個裝置上。GPipe 引入了**管線平行化**，結合**微批次**和**重新實體化**，以高效訓練無法放入單一加速器的模型。\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "### 1. 管線平行化\n",
    "- 將模型分割為 **K 個分區**，分布在 K 個裝置上\n",
    "- 每個裝置持有連續的層\n",
    "- 資料流經管線：裝置 1 → 裝置 2 → ... → 裝置 K\n",
    "\n",
    "### 2. 微批次處理\n",
    "- 將大小為 N 的小批次分割為 M 個微批次，每個大小為 N/M\n",
    "- 微批次依序通過管線處理\n",
    "- **減少氣泡時間**（裝置閒置時間）\n",
    "\n",
    "### 3. F-then-B 排程\n",
    "```\n",
    "先前向所有 M 個微批次，再反向所有 M 個微批次\n",
    "裝置 1: F1 F2 F3 F4 ........... B4 B3 B2 B1\n",
    "裝置 2: .. F1 F2 F3 F4 ....... B4 B3 B2 B1\n",
    "裝置 3: .... F1 F2 F3 F4 ..... B4 B3 B2 B1\n",
    "裝置 4: ...... F1 F2 F3 F4 ... B4 B3 B2 B1\n",
    "```\n",
    "\n",
    "### 4. 重新實體化（梯度檢查點）\n",
    "- 不儲存所有激活值（記憶體密集）\n",
    "- 只在分區邊界設置檢查點\n",
    "- 在反向傳遞時重新計算中間激活值\n",
    "- **用計算換取記憶體**\n",
    "\n",
    "### 5. 氣泡時間\n",
    "- 裝置閒置時間的比例：**(K-1) / (K-1 + M)**\n",
    "- 更多微批次 M → 更少氣泡時間\n",
    "- 更多裝置 K → 更多氣泡時間\n",
    "\n",
    "---\n",
    "\n",
    "## 實作概述\n",
    "\n",
    "我們將實作：\n",
    "1. 在「模擬」裝置上進行模型分區\n",
    "2. 微批次分割和排程\n",
    "3. 通過管線的前向和反向傳遞\n",
    "4. 梯度累積\n",
    "5. 記憶體效率的重新實體化\n",
    "6. 與資料平行化的比較\n",
    "7. 氣泡時間分析\n",
    "\n",
    "讓我們開始建構！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"程式庫匯入成功！\")\n",
    "print(\"NumPy 版本:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一節：模型分區和管線結構\n",
    "\n",
    "GPipe 的第一步是將大型模型分區為 K 個段落，每個分配給不同的裝置。\n",
    "\n",
    "## 分區策略\n",
    "\n",
    "對於有 L 層的模型：\n",
    "- **均勻分區**：每個分區獲得約 L/K 層\n",
    "- **平衡分區**：按計算時間或記憶體進行分區\n",
    "\n",
    "我們將實作一個簡單的多層網路並進行均勻分區。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Layer:\n",
    "    \"\"\"單一神經網路層。\"\"\"\n",
    "    W: np.ndarray  # 權重矩陣\n",
    "    b: np.ndarray  # 偏置向量\n",
    "    activation: str = 'relu'  # 'relu'、'tanh' 或 'linear'\n",
    "    \n",
    "    def forward(self, x: np.ndarray, store_activation: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"前向傳遞：z = W @ x + b, a = activation(z)\"\"\"\n",
    "        z = x @ self.W + self.b  # 線性變換\n",
    "        \n",
    "        # 應用激活函數\n",
    "        if self.activation == 'relu':\n",
    "            a = np.maximum(0, z)\n",
    "        elif self.activation == 'tanh':\n",
    "            a = np.tanh(z)\n",
    "        elif self.activation == 'linear':\n",
    "            a = z\n",
    "        else:\n",
    "            raise ValueError(f\"未知的激活函數：{self.activation}\")\n",
    "        \n",
    "        return a, z if store_activation else None\n",
    "    \n",
    "    def backward(self, da: np.ndarray, z: np.ndarray, x: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"反向傳遞：計算梯度。\"\"\"\n",
    "        # 激活函數梯度\n",
    "        if self.activation == 'relu':\n",
    "            dz = da * (z > 0)\n",
    "        elif self.activation == 'tanh':\n",
    "            dz = da * (1 - np.tanh(z)**2)\n",
    "        elif self.activation == 'linear':\n",
    "            dz = da\n",
    "        else:\n",
    "            raise ValueError(f\"未知的激活函數：{self.activation}\")\n",
    "        \n",
    "        # 參數梯度\n",
    "        dW = x.T @ dz\n",
    "        db = np.sum(dz, axis=0)\n",
    "        \n",
    "        # 輸入梯度（用於前一層）\n",
    "        dx = dz @ self.W.T\n",
    "        \n",
    "        return dx, dW, db\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Partition:\n",
    "    \"\"\"模型的一個分區（分配給一個裝置的層子集）。\"\"\"\n",
    "    device_id: int\n",
    "    layers: List[Layer]\n",
    "    \n",
    "    def forward(self, x: np.ndarray, store_activations: bool = True) -> Tuple[np.ndarray, List[Tuple]]:\n",
    "        \"\"\"此分區中所有層的前向傳遞。\"\"\"\n",
    "        activations = []  # 如需要，儲存每層的 (x, z)\n",
    "        \n",
    "        current = x\n",
    "        for layer in self.layers:\n",
    "            if store_activations:\n",
    "                activations.append(current)  # 儲存此層的輸入\n",
    "            \n",
    "            current, z = layer.forward(current, store_activation=store_activations)\n",
    "            \n",
    "            if store_activations:\n",
    "                activations.append(z)  # 儲存激活前的值\n",
    "        \n",
    "        return current, activations\n",
    "    \n",
    "    def backward(self, dout: np.ndarray, activations: List) -> Tuple[np.ndarray, List[Tuple]]:\n",
    "        \"\"\"此分區中所有層的反向傳遞。\"\"\"\n",
    "        gradients = []  # 儲存每層的 (dW, db)\n",
    "        \n",
    "        da = dout\n",
    "        # 反向遍歷各層\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            \n",
    "            # 獲取儲存的激活值\n",
    "            x = activations[2*i]      # 此層的輸入\n",
    "            z = activations[2*i + 1]  # 激活前的值\n",
    "            \n",
    "            # 計算梯度\n",
    "            da, dW, db = layer.backward(da, z, x)\n",
    "            gradients.insert(0, (dW, db))\n",
    "        \n",
    "        return da, gradients  # da 是相對於分區輸入的梯度\n",
    "\n",
    "\n",
    "def create_model(layer_dims: List[int], activations: List[str]) -> List[Layer]:\n",
    "    \"\"\"建立多層神經網路。\n",
    "    \n",
    "    參數：\n",
    "        layer_dims: [input_dim, hidden1, hidden2, ..., output_dim]\n",
    "        activations: 每層的激活函數\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(len(layer_dims) - 1):\n",
    "        W = np.random.randn(layer_dims[i], layer_dims[i+1]) * np.sqrt(2.0 / layer_dims[i])\n",
    "        b = np.zeros(layer_dims[i+1])\n",
    "        layers.append(Layer(W, b, activations[i]))\n",
    "    return layers\n",
    "\n",
    "\n",
    "def partition_model(layers: List[Layer], num_partitions: int) -> List[Partition]:\n",
    "    \"\"\"將層均勻分區到各裝置。\"\"\"\n",
    "    num_layers = len(layers)\n",
    "    layers_per_partition = num_layers // num_partitions\n",
    "    \n",
    "    partitions = []\n",
    "    for k in range(num_partitions):\n",
    "        start = k * layers_per_partition\n",
    "        if k == num_partitions - 1:\n",
    "            # 最後一個分區獲得剩餘的層\n",
    "            end = num_layers\n",
    "        else:\n",
    "            end = (k + 1) * layers_per_partition\n",
    "        \n",
    "        partition_layers = layers[start:end]\n",
    "        partitions.append(Partition(device_id=k, layers=partition_layers))\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "\n",
    "# 範例：建立並分區一個 12 層網路\n",
    "layer_dims = [128] + [256] * 10 + [10]  # 輸入=128，10 個 256 的隱藏層，輸出=10\n",
    "activations = ['relu'] * 10 + ['linear']  # 隱藏層用 ReLU，輸出層用 linear\n",
    "\n",
    "model_layers = create_model(layer_dims, activations)\n",
    "print(f\"建立了 {len(model_layers)} 層的模型\")\n",
    "\n",
    "# 分區到 4 個「裝置」\n",
    "K = 4\n",
    "partitions = partition_model(model_layers, K)\n",
    "\n",
    "print(f\"\\n將模型分區為 {K} 個分區：\")\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"  裝置 {i}：{len(partition.layers)} 層\")\n",
    "\n",
    "print(\"\\n✓ 模型分區完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二節：微批次策略\n",
    "\n",
    "GPipe 將每個小批次分割為 M 個**微批次**以提高管線利用率。\n",
    "\n",
    "## 為什麼要微批次處理？\n",
    "\n",
    "沒有微批次處理：\n",
    "```\n",
    "裝置 1: [前向] .................... [反向]\n",
    "裝置 2:          [前向] .......... [反向]\n",
    "裝置 3:                   [前向] [反向]\n",
    "          ^^^^^^^^                     ^^^^^^^^^^\n",
    "          氣泡                         氣泡\n",
    "```\n",
    "\n",
    "有 M 個微批次：\n",
    "```\n",
    "裝置 1: F1 F2 F3 F4 ........... B4 B3 B2 B1\n",
    "裝置 2:    F1 F2 F3 F4 ....... B4 B3 B2 B1\n",
    "裝置 3:       F1 F2 F3 F4 .... B4 B3 B2 B1\n",
    "          ^^                              ^^\n",
    "          更小的氣泡\n",
    "```\n",
    "\n",
    "**氣泡比例**：(K-1) / (K-1 + M)\n",
    "- 更多微批次 → 更少氣泡時間\n",
    "- 但更多微批次 → 更多開銷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_microbatches(X: np.ndarray, y: np.ndarray, num_microbatches: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"將小批次分割為微批次。\n",
    "    \n",
    "    參數：\n",
    "        X: 輸入資料 (batch_size, features)\n",
    "        y: 標籤 (batch_size, ...)\n",
    "        num_microbatches: M（微批次數量）\n",
    "    \n",
    "    返回：\n",
    "        (X_micro, y_micro) 元組的列表\n",
    "    \"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    microbatch_size = batch_size // num_microbatches\n",
    "    \n",
    "    if batch_size % num_microbatches != 0:\n",
    "        raise ValueError(f\"批次大小 {batch_size} 必須能被微批次數 {num_microbatches} 整除\")\n",
    "    \n",
    "    microbatches = []\n",
    "    for m in range(num_microbatches):\n",
    "        start = m * microbatch_size\n",
    "        end = (m + 1) * microbatch_size\n",
    "        microbatches.append((X[start:end], y[start:end]))\n",
    "    \n",
    "    return microbatches\n",
    "\n",
    "\n",
    "def compute_bubble_fraction(K: int, M: int) -> float:\n",
    "    \"\"\"GPipe 的理論氣泡比例。\n",
    "    \n",
    "    公式：(K - 1) / (K - 1 + M)\n",
    "    \n",
    "    參數：\n",
    "        K: 裝置/分區數量\n",
    "        M: 微批次數量\n",
    "    \"\"\"\n",
    "    return (K - 1) / (K - 1 + M)\n",
    "\n",
    "\n",
    "# 範例：分析氣泡比例\n",
    "K_values = [2, 4, 8, 16]\n",
    "M_values = [1, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "print(\"氣泡比例分析：\")\n",
    "print(\"\\nM（微批次數）→\")\n",
    "print(\"K ↓\\t\" + \"\\t\".join(f\"{M:d}\" for M in M_values))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for K in K_values:\n",
    "    row = f\"{K}\\t\"\n",
    "    for M in M_values:\n",
    "        bubble = compute_bubble_fraction(K, M)\n",
    "        row += f\"{bubble:.3f}\\t\"\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n關鍵觀察：\")\n",
    "print(\"  - 更多裝置 (K) → 更多氣泡時間（裝置等待管線）\")\n",
    "print(\"  - 更多微批次 (M) → 更少氣泡時間（管線保持滿載）\")\n",
    "print(\"  - K=4, M=8 時：氣泡比例 = 27.3%（裝置 27% 時間閒置）\")\n",
    "print(\"  - K=4, M=32 時：氣泡比例 = 8.6%（好很多！）\")\n",
    "\n",
    "# 微批次處理範例\n",
    "batch_size = 32\n",
    "M = 8\n",
    "X_batch = np.random.randn(batch_size, 128)\n",
    "y_batch = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "microbatches = split_into_microbatches(X_batch, y_batch, M)\n",
    "print(f\"\\n\\n將 {batch_size} 的批次分割為 {M} 個微批次：\")\n",
    "for i, (X_m, y_m) in enumerate(microbatches):\n",
    "    print(f\"  微批次 {i}：X 形狀 {X_m.shape}，y 形狀 {y_m.shape}\")\n",
    "\n",
    "print(\"\\n✓ 微批次處理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三節：通過管線的前向傳遞（F-then-B 排程）\n",
    "\n",
    "GPipe 使用 **F-then-B 排程**：\n",
    "1. 將所有 M 個微批次前向通過管線\n",
    "2. 將所有 M 個微批次反向通過管線（以相反順序）\n",
    "\n",
    "## 時間線範例（K=3 裝置，M=4 微批次）：\n",
    "\n",
    "```\n",
    "時間 →  0   1   2   3   4   5   6   7   8   9   10  11  12\n",
    "裝置 0:  F0  F1  F2  F3  ... ... ... B3  B2  B1  B0\n",
    "裝置 1:  ... F0  F1  F2  F3  ... ... ... B3  B2  B1  B0\n",
    "裝置 2:  ... ... F0  F1  F2  F3  ... ... ... B3  B2  B1  B0\n",
    "```\n",
    "\n",
    "說明：\n",
    "- **F0** = 微批次 0 前向\n",
    "- **B3** = 微批次 3 反向\n",
    "- **...** = 氣泡（裝置閒置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineEvent:\n",
    "    \"\"\"記錄裝置何時執行操作。\"\"\"\n",
    "    time_step: int\n",
    "    device_id: int\n",
    "    operation: str  # 'forward' 或 'backward'\n",
    "    microbatch_id: int\n",
    "\n",
    "\n",
    "class GPipePipeline:\n",
    "    \"\"\"使用 F-then-B 排程的 GPipe 管線。\"\"\"\n",
    "    \n",
    "    def __init__(self, partitions: List[Partition]):\n",
    "        self.partitions = partitions\n",
    "        self.K = len(partitions)  # 裝置數量\n",
    "        \n",
    "        # 用於追蹤執行時間線\n",
    "        self.events = []  # PipelineEvent 列表\n",
    "    \n",
    "    def forward_pipeline(self, microbatches: List[Tuple[np.ndarray, np.ndarray]], \n",
    "                        store_activations: bool = True) -> Tuple[List[np.ndarray], List[List]]:\n",
    "        \"\"\"前向傳遞：處理所有微批次通過管線。\n",
    "        \n",
    "        返回：\n",
    "            outputs: 每個微批次的最終輸出列表\n",
    "            all_activations: 激活值列表的列表（每個微批次一個）\n",
    "        \"\"\"\n",
    "        M = len(microbatches)\n",
    "        \n",
    "        # 輸出和激活值的儲存\n",
    "        outputs = [None] * M\n",
    "        all_activations = [[None] * self.K for _ in range(M)]  # [微批次][分區]\n",
    "        \n",
    "        # F-then-B 排程：前向所有微批次\n",
    "        time_step = 0\n",
    "        \n",
    "        for m in range(M):\n",
    "            X_micro, y_micro = microbatches[m]\n",
    "            current = X_micro\n",
    "            \n",
    "            # 通過每個分區前向\n",
    "            for k, partition in enumerate(self.partitions):\n",
    "                self.events.append(PipelineEvent(time_step, k, 'forward', m))\n",
    "                \n",
    "                current, activations = partition.forward(current, store_activations)\n",
    "                all_activations[m][k] = activations\n",
    "                \n",
    "                time_step += 1\n",
    "            \n",
    "            outputs[m] = current\n",
    "        \n",
    "        return outputs, all_activations\n",
    "    \n",
    "    def backward_pipeline(self, outputs: List[np.ndarray], \n",
    "                         labels: List[np.ndarray],\n",
    "                         all_activations: List[List]) -> List[List[List[Tuple]]]:\n",
    "        \"\"\"反向傳遞：以相反順序處理所有微批次。\n",
    "        \n",
    "        返回：\n",
    "            all_gradients: [微批次][分區][(dW, db) 每層]\n",
    "        \"\"\"\n",
    "        M = len(outputs)\n",
    "        \n",
    "        # 梯度的儲存\n",
    "        all_gradients = [[None] * self.K for _ in range(M)]\n",
    "        \n",
    "        # 找到當前時間步（在前向傳遞之後）\n",
    "        time_step = max(e.time_step for e in self.events) + 1\n",
    "        \n",
    "        # 以相反順序反向所有微批次\n",
    "        for m in range(M - 1, -1, -1):\n",
    "            # 計算損失梯度（簡單 MSE 示範）\n",
    "            dout = 2 * (outputs[m] - labels[m]) / labels[m].shape[0]\n",
    "            \n",
    "            # 以相反順序通過每個分區反向\n",
    "            for k in range(self.K - 1, -1, -1):\n",
    "                partition = self.partitions[k]\n",
    "                activations = all_activations[m][k]\n",
    "                \n",
    "                self.events.append(PipelineEvent(time_step, k, 'backward', m))\n",
    "                \n",
    "                dout, gradients = partition.backward(dout, activations)\n",
    "                all_gradients[m][k] = gradients\n",
    "                \n",
    "                time_step += 1\n",
    "        \n",
    "        return all_gradients\n",
    "    \n",
    "    def get_timeline_matrix(self) -> np.ndarray:\n",
    "        \"\"\"將事件轉換為 K×T 矩陣以供視覺化。\n",
    "        \n",
    "        矩陣值：\n",
    "            0 = 氣泡（閒置）\n",
    "            m+1 = 微批次 m 前向\n",
    "            -(m+1) = 微批次 m 反向\n",
    "        \"\"\"\n",
    "        max_time = max(e.time_step for e in self.events) + 1\n",
    "        timeline = np.zeros((self.K, max_time))\n",
    "        \n",
    "        for event in self.events:\n",
    "            value = event.microbatch_id + 1\n",
    "            if event.operation == 'backward':\n",
    "                value = -value\n",
    "            timeline[event.device_id, event.time_step] = value\n",
    "        \n",
    "        return timeline\n",
    "\n",
    "\n",
    "# 測試前向傳遞\n",
    "print(\"測試 GPipe 前向傳遞...\\n\")\n",
    "\n",
    "# 建立管線\n",
    "pipeline = GPipePipeline(partitions)\n",
    "\n",
    "# 建立微批次\n",
    "M = 4\n",
    "batch_size = 16\n",
    "X_batch = np.random.randn(batch_size, 128)\n",
    "y_batch_onehot = np.eye(10)[np.random.randint(0, 10, batch_size)]\n",
    "\n",
    "microbatches = split_into_microbatches(X_batch, y_batch_onehot, M)\n",
    "\n",
    "# 前向傳遞\n",
    "outputs, all_activations = pipeline.forward_pipeline(microbatches)\n",
    "\n",
    "print(f\"處理了 {M} 個微批次通過 {pipeline.K} 個裝置\")\n",
    "print(f\"輸出形狀：{[out.shape for out in outputs]}\")\n",
    "print(f\"前向事件總數：{len([e for e in pipeline.events if e.operation == 'forward'])}\")\n",
    "\n",
    "# 反向傳遞\n",
    "labels = [mb[1] for mb in microbatches]\n",
    "all_gradients = pipeline.backward_pipeline(outputs, labels, all_activations)\n",
    "\n",
    "print(f\"反向事件總數：{len([e for e in pipeline.events if e.operation == 'backward'])}\")\n",
    "print(f\"\\n總時間步數：{max(e.time_step for e in pipeline.events) + 1}\")\n",
    "\n",
    "print(\"\\n✓ 管線前向和反向傳遞完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四節：跨微批次的梯度累積\n",
    "\n",
    "處理完所有 M 個微批次後，我們需要：\n",
    "1. **累積梯度**來自所有微批次\n",
    "2. **平均**它們（因為它們來自同一個小批次）\n",
    "3. **應用**累積的梯度來更新參數\n",
    "\n",
    "這等同於一次處理整個小批次，但管線利用率更好！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_gradients(all_gradients: List[List[List[Tuple]]]) -> List[List[Tuple]]:\n",
    "    \"\"\"累積並平均所有微批次的梯度。\n",
    "    \n",
    "    參數：\n",
    "        all_gradients: [微批次][分區][(dW, db) 每層]\n",
    "    \n",
    "    返回：\n",
    "        accumulated: [分區][(dW, db) 每層] - 跨微批次平均\n",
    "    \"\"\"\n",
    "    M = len(all_gradients)  # 微批次數量\n",
    "    K = len(all_gradients[0])  # 分區數量\n",
    "    \n",
    "    # 初始化累積梯度（從第一個微批次複製結構）\n",
    "    accumulated = []\n",
    "    for k in range(K):\n",
    "        partition_grads = []\n",
    "        for layer_idx in range(len(all_gradients[0][k])):\n",
    "            # 跨微批次求和梯度\n",
    "            dW_sum = sum(all_gradients[m][k][layer_idx][0] for m in range(M))\n",
    "            db_sum = sum(all_gradients[m][k][layer_idx][1] for m in range(M))\n",
    "            \n",
    "            # 平均（因為微批次是同一個小批次的一部分）\n",
    "            dW_avg = dW_sum / M\n",
    "            db_avg = db_sum / M\n",
    "            \n",
    "            partition_grads.append((dW_avg, db_avg))\n",
    "        \n",
    "        accumulated.append(partition_grads)\n",
    "    \n",
    "    return accumulated\n",
    "\n",
    "\n",
    "def apply_gradients(partitions: List[Partition], gradients: List[List[Tuple]], learning_rate: float):\n",
    "    \"\"\"應用累積的梯度來更新參數。\n",
    "    \n",
    "    參數：\n",
    "        partitions: 模型分區列表\n",
    "        gradients: [分區][(dW, db) 每層]\n",
    "        learning_rate: SGD 的學習率\n",
    "    \"\"\"\n",
    "    for k, partition in enumerate(partitions):\n",
    "        partition_grads = gradients[k]\n",
    "        \n",
    "        for layer_idx, layer in enumerate(partition.layers):\n",
    "            dW, db = partition_grads[layer_idx]\n",
    "            \n",
    "            # SGD 更新\n",
    "            layer.W -= learning_rate * dW\n",
    "            layer.b -= learning_rate * db\n",
    "\n",
    "\n",
    "# 測試梯度累積\n",
    "print(\"測試梯度累積...\\n\")\n",
    "\n",
    "# 我們已經有了前一個 cell 的 all_gradients\n",
    "accumulated_grads = accumulate_gradients(all_gradients)\n",
    "\n",
    "print(f\"為 {len(accumulated_grads)} 個分區累積了梯度：\")\n",
    "for k, partition_grads in enumerate(accumulated_grads):\n",
    "    print(f\"  分區 {k}：{len(partition_grads)} 層\")\n",
    "    for i, (dW, db) in enumerate(partition_grads[:2]):  # 顯示前 2 層\n",
    "        print(f\"    層 {i}：dW 形狀 {dW.shape}，db 形狀 {db.shape}\")\n",
    "        print(f\"             dW 範數：{np.linalg.norm(dW):.6f}，db 範數：{np.linalg.norm(db):.6f}\")\n",
    "\n",
    "# 應用梯度\n",
    "learning_rate = 0.01\n",
    "old_W = partitions[0].layers[0].W.copy()\n",
    "\n",
    "apply_gradients(partitions, accumulated_grads, learning_rate)\n",
    "\n",
    "new_W = partitions[0].layers[0].W\n",
    "weight_change = np.linalg.norm(new_W - old_W)\n",
    "\n",
    "print(f\"\\n以學習率 {learning_rate} 應用了梯度\")\n",
    "print(f\"權重變化（第一層）：{weight_change:.6f}\")\n",
    "\n",
    "print(\"\\n✓ 梯度累積和應用完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五節：重新實體化（梯度檢查點）\n",
    "\n",
    "**問題**：儲存所有 M 個微批次在 K 個分區的激活值需要 O(M × K × layer_memory) 的記憶體。\n",
    "\n",
    "**解決方案**：**重新實體化**（梯度檢查點）\n",
    "- 只在**分區邊界**設置激活值檢查點\n",
    "- 在反向傳遞時**重新計算**中間激活值\n",
    "- 權衡：約 33% 額外計算換取約 K 倍更少記憶體\n",
    "\n",
    "## 記憶體比較\n",
    "\n",
    "**沒有重新實體化**：\n",
    "- 儲存所有分區所有層的激活值\n",
    "- 記憶體：O(M × L)，其中 L = 總層數\n",
    "\n",
    "**有重新實體化**：\n",
    "- 只儲存分區邊界的激活值\n",
    "- 記憶體：O(M × K)，其中 K = 分區數（K << L）\n",
    "- 根據需要重新計算中間激活值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPipePipelineWithRemat:\n",
    "    \"\"\"帶重新實體化（梯度檢查點）的 GPipe。\"\"\"\n",
    "    \n",
    "    def __init__(self, partitions: List[Partition]):\n",
    "        self.partitions = partitions\n",
    "        self.K = len(partitions)\n",
    "        self.events = []\n",
    "    \n",
    "    def forward_pipeline_remat(self, microbatches: List[Tuple[np.ndarray, np.ndarray]]) -> Tuple[List, List]:\n",
    "        \"\"\"帶重新實體化的前向傳遞：只儲存分區邊界激活值。\n",
    "        \n",
    "        返回：\n",
    "            outputs: 每個微批次的最終輸出\n",
    "            boundary_inputs: 每個分區的輸入（用於重新計算）\n",
    "        \"\"\"\n",
    "        M = len(microbatches)\n",
    "        \n",
    "        outputs = [None] * M\n",
    "        # 只儲存每個分區的輸入（邊界激活值）\n",
    "        boundary_inputs = [[None] * self.K for _ in range(M)]\n",
    "        \n",
    "        time_step = 0\n",
    "        \n",
    "        for m in range(M):\n",
    "            X_micro, y_micro = microbatches[m]\n",
    "            current = X_micro\n",
    "            \n",
    "            for k, partition in enumerate(self.partitions):\n",
    "                # 儲存此分區的輸入（邊界）\n",
    "                boundary_inputs[m][k] = current.copy()\n",
    "                \n",
    "                self.events.append(PipelineEvent(time_step, k, 'forward', m))\n",
    "                \n",
    "                # 前向傳遞，不儲存中間激活值\n",
    "                current, _ = partition.forward(current, store_activations=False)\n",
    "                \n",
    "                time_step += 1\n",
    "            \n",
    "            outputs[m] = current\n",
    "        \n",
    "        return outputs, boundary_inputs\n",
    "    \n",
    "    def backward_pipeline_remat(self, outputs: List[np.ndarray],\n",
    "                                labels: List[np.ndarray],\n",
    "                                boundary_inputs: List[List]) -> List[List[List[Tuple]]]:\n",
    "        \"\"\"帶重新實體化的反向傳遞：根據需要重新計算激活值。\"\"\"\n",
    "        M = len(outputs)\n",
    "        all_gradients = [[None] * self.K for _ in range(M)]\n",
    "        \n",
    "        time_step = max(e.time_step for e in self.events) + 1\n",
    "        \n",
    "        for m in range(M - 1, -1, -1):\n",
    "            dout = 2 * (outputs[m] - labels[m]) / labels[m].shape[0]\n",
    "            \n",
    "            for k in range(self.K - 1, -1, -1):\n",
    "                partition = self.partitions[k]\n",
    "                \n",
    "                self.events.append(PipelineEvent(time_step, k, 'backward', m))\n",
    "                \n",
    "                # 重新計算此分區的激活值\n",
    "                partition_input = boundary_inputs[m][k]\n",
    "                _, activations = partition.forward(partition_input, store_activations=True)\n",
    "                \n",
    "                # 現在使用重新計算的激活值計算梯度\n",
    "                dout, gradients = partition.backward(dout, activations)\n",
    "                all_gradients[m][k] = gradients\n",
    "                \n",
    "                time_step += 1\n",
    "        \n",
    "        return all_gradients\n",
    "\n",
    "\n",
    "def estimate_memory_usage(M: int, K: int, layers_per_partition: int, \n",
    "                         activation_size_mb: float, with_remat: bool) -> float:\n",
    "    \"\"\"估計有無重新實體化的記憶體使用量。\n",
    "    \n",
    "    參數：\n",
    "        M: 微批次數量\n",
    "        K: 分區數量\n",
    "        layers_per_partition: 每分區平均層數\n",
    "        activation_size_mb: 一層激活值的記憶體（MB）\n",
    "        with_remat: 是否使用重新實體化？\n",
    "    \n",
    "    返回：\n",
    "        估計的記憶體（MB）\n",
    "    \"\"\"\n",
    "    if with_remat:\n",
    "        # 只儲存邊界輸入（每個微批次 K 個）\n",
    "        return M * K * activation_size_mb\n",
    "    else:\n",
    "        # 儲存所有中間激活值\n",
    "        total_layers = K * layers_per_partition\n",
    "        return M * total_layers * activation_size_mb\n",
    "\n",
    "\n",
    "# 測試重新實體化\n",
    "print(\"測試重新實體化...\\n\")\n",
    "\n",
    "# 建立帶重新實體化的新管線\n",
    "pipeline_remat = GPipePipelineWithRemat(partitions)\n",
    "\n",
    "# 帶重新實體化的前向\n",
    "outputs_remat, boundary_inputs = pipeline_remat.forward_pipeline_remat(microbatches)\n",
    "\n",
    "print(\"帶重新實體化的前向傳遞：\")\n",
    "print(f\"  儲存的邊界輸入：{len(boundary_inputs)} 個微批次 × {len(boundary_inputs[0])} 個分區\")\n",
    "print(f\"  邊界輸入形狀：{[bi[0].shape for bi in boundary_inputs]}\")\n",
    "\n",
    "# 帶重新實體化的反向\n",
    "gradients_remat = pipeline_remat.backward_pipeline_remat(outputs_remat, labels, boundary_inputs)\n",
    "\n",
    "print(f\"\\n帶重新實體化的反向傳遞：\")\n",
    "print(f\"  計算的梯度：{len(gradients_remat)} 個微批次 × {len(gradients_remat[0])} 個分區\")\n",
    "\n",
    "# 記憶體分析\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"記憶體使用量比較\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "M_test = 8\n",
    "K_test = 4\n",
    "layers_per_partition = 3\n",
    "activation_size_mb = 10  # 每層激活值 MB\n",
    "\n",
    "mem_without = estimate_memory_usage(M_test, K_test, layers_per_partition, activation_size_mb, with_remat=False)\n",
    "mem_with = estimate_memory_usage(M_test, K_test, layers_per_partition, activation_size_mb, with_remat=True)\n",
    "\n",
    "print(f\"\\n配置：M={M_test}，K={K_test}，每分區 {layers_per_partition} 層\")\n",
    "print(f\"  無重新實體化：{mem_without:.1f} MB\")\n",
    "print(f\"  有重新實體化：{mem_with:.1f} MB\")\n",
    "print(f\"  記憶體節省：    {mem_without / mem_with:.1f}×\")\n",
    "\n",
    "print(\"\\n✓ 重新實體化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第六節：管線排程視覺化和氣泡分析\n",
    "\n",
    "讓我們視覺化 F-then-B 排程並量化氣泡時間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pipeline_schedule(pipeline: GPipePipeline, title: str = \"GPipe 排程（F-then-B）\"):\n",
    "    \"\"\"視覺化管線執行時間線。\"\"\"\n",
    "    timeline = pipeline.get_timeline_matrix()\n",
    "    K, T = timeline.shape\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # 建立顏色映射\n",
    "    # 正值 = 前向（暖色），負值 = 反向（冷色），0 = 氣泡（白色）\n",
    "    M = int(np.max(np.abs(timeline)))\n",
    "    colors_forward = plt.cm.Reds(np.linspace(0.3, 0.9, M))\n",
    "    colors_backward = plt.cm.Blues(np.linspace(0.3, 0.9, M))\n",
    "    \n",
    "    # 繪製時間線\n",
    "    for k in range(K):\n",
    "        for t in range(T):\n",
    "            val = timeline[k, t]\n",
    "            if val > 0:  # 前向\n",
    "                color = colors_forward[int(val) - 1]\n",
    "                label = f'F{int(val)-1}'\n",
    "            elif val < 0:  # 反向\n",
    "                color = colors_backward[int(-val) - 1]\n",
    "                label = f'B{int(-val)-1}'\n",
    "            else:  # 氣泡\n",
    "                color = 'white'\n",
    "                label = ''\n",
    "            \n",
    "            rect = plt.Rectangle((t, k), 1, 1, facecolor=color, edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            if label:\n",
    "                ax.text(t + 0.5, k + 0.5, label, ha='center', va='center', \n",
    "                       fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, T)\n",
    "    ax.set_ylim(0, K)\n",
    "    ax.set_xlabel('時間步', fontsize=12)\n",
    "    ax.set_ylabel('裝置', fontsize=12)\n",
    "    ax.set_yticks(np.arange(K) + 0.5)\n",
    "    ax.set_yticklabels([f'裝置 {k}' for k in range(K)])\n",
    "    ax.set_xticks(np.arange(T) + 0.5)\n",
    "    ax.set_xticklabels(np.arange(T))\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # 添加圖例\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='salmon', label='前向傳遞'),\n",
    "        Patch(facecolor='lightblue', label='反向傳遞'),\n",
    "        Patch(facecolor='white', edgecolor='black', label='氣泡（閒置）')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_actual_bubble_time(timeline: np.ndarray) -> float:\n",
    "    \"\"\"從時間線計算實際氣泡比例。\"\"\"\n",
    "    total_steps = timeline.size\n",
    "    bubble_steps = np.sum(timeline == 0)\n",
    "    return bubble_steps / total_steps\n",
    "\n",
    "\n",
    "# 視覺化我們之前建立的管線\n",
    "print(\"視覺化 GPipe 管線排程...\\n\")\n",
    "\n",
    "visualize_pipeline_schedule(pipeline_remat, f\"GPipe：K={K} 個裝置，M={M} 個微批次\")\n",
    "\n",
    "# 分析氣泡時間\n",
    "timeline = pipeline_remat.get_timeline_matrix()\n",
    "actual_bubble = compute_actual_bubble_time(timeline)\n",
    "theoretical_bubble = compute_bubble_fraction(K, M)\n",
    "\n",
    "print(f\"\\n氣泡時間分析（K={K}，M={M}）：\")\n",
    "print(f\"  理論氣泡比例：{theoretical_bubble:.3f}（{theoretical_bubble*100:.1f}%）\")\n",
    "print(f\"  實際氣泡比例：{actual_bubble:.3f}（{actual_bubble*100:.1f}%）\")\n",
    "print(f\"  管線效率：    {(1-actual_bubble)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n✓ 排程視覺化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第七節：比較 - 管線平行化 vs 資料平行化\n",
    "\n",
    "讓我們比較 GPipe（管線平行化）與傳統資料平行化。\n",
    "\n",
    "## 資料平行化\n",
    "- 在每個裝置上複製整個模型\n",
    "- 將批次分割到各裝置\n",
    "- 同步梯度（all-reduce）\n",
    "- **限制**：模型必須能放入單一裝置\n",
    "\n",
    "## 管線平行化（GPipe）\n",
    "- 將模型分割到各裝置\n",
    "- 所有裝置處理同一批次（不同微批次）\n",
    "- 不需要梯度同步\n",
    "- **優勢**：可以訓練比單一裝置記憶體更大的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_parallelism(model_layers: List[Layer], \n",
    "                             batch_size: int, \n",
    "                             num_devices: int) -> Dict[str, float]:\n",
    "    \"\"\"模擬資料平行化時間。\n",
    "    \n",
    "    返回：\n",
    "        包含時間分解的字典\n",
    "    \"\"\"\n",
    "    # 每個裝置處理 batch_size/num_devices 個樣本\n",
    "    local_batch_size = batch_size // num_devices\n",
    "    \n",
    "    # 時間（任意單位）\n",
    "    forward_time = len(model_layers) * 1.0  # 每層一個單位\n",
    "    backward_time = len(model_layers) * 1.0\n",
    "    allreduce_time = 2.0  # 通訊開銷\n",
    "    \n",
    "    total_time = forward_time + backward_time + allreduce_time\n",
    "    \n",
    "    return {\n",
    "        'forward': forward_time,\n",
    "        'backward': backward_time,\n",
    "        'communication': allreduce_time,\n",
    "        'total': total_time,\n",
    "        'efficiency': (forward_time + backward_time) / total_time\n",
    "    }\n",
    "\n",
    "\n",
    "def simulate_pipeline_parallelism(model_layers: List[Layer],\n",
    "                                 batch_size: int,\n",
    "                                 num_devices: int,\n",
    "                                 num_microbatches: int) -> Dict[str, float]:\n",
    "    \"\"\"模擬管線平行化時間。\"\"\"\n",
    "    layers_per_device = len(model_layers) // num_devices\n",
    "    \n",
    "    # 一個微批次通過一個分區的時間\n",
    "    forward_time_per_micro = layers_per_device * 1.0\n",
    "    backward_time_per_micro = layers_per_device * 1.0\n",
    "    \n",
    "    # 總管線時間\n",
    "    # 填充管線：(K-1) + M 個微批次\n",
    "    # 每步：通過一個分區前向或反向\n",
    "    total_forward_steps = (num_devices - 1) + num_microbatches\n",
    "    total_backward_steps = (num_devices - 1) + num_microbatches\n",
    "    \n",
    "    total_time = (total_forward_steps + total_backward_steps) * layers_per_device\n",
    "    \n",
    "    # 計算時間（排除氣泡）\n",
    "    compute_time = 2 * num_microbatches * layers_per_device * num_devices\n",
    "    \n",
    "    return {\n",
    "        'forward': total_forward_steps * layers_per_device,\n",
    "        'backward': total_backward_steps * layers_per_device,\n",
    "        'communication': 0,  # 沒有裝置間通訊！\n",
    "        'total': total_time,\n",
    "        'efficiency': compute_time / (total_time * num_devices),\n",
    "        'bubble_fraction': compute_bubble_fraction(num_devices, num_microbatches)\n",
    "    }\n",
    "\n",
    "\n",
    "# 比較兩種方法\n",
    "print(\"比較管線平行化 vs 資料平行化\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_layers = 12\n",
    "batch_size = 32\n",
    "num_devices = 4\n",
    "num_microbatches = 8\n",
    "\n",
    "# 模擬資料平行化\n",
    "data_parallel_stats = simulate_data_parallelism(model_layers, batch_size, num_devices)\n",
    "\n",
    "print(\"資料平行化：\")\n",
    "print(f\"  配置：{num_devices} 個裝置，批次大小 {batch_size}\")\n",
    "print(f\"  前向時間：       {data_parallel_stats['forward']:.1f} 單位\")\n",
    "print(f\"  反向時間：       {data_parallel_stats['backward']:.1f} 單位\")\n",
    "print(f\"  通訊時間：       {data_parallel_stats['communication']:.1f} 單位（all-reduce）\")\n",
    "print(f\"  總時間：         {data_parallel_stats['total']:.1f} 單位\")\n",
    "print(f\"  效率：           {data_parallel_stats['efficiency']*100:.1f}%\")\n",
    "print(f\"  ⚠️  限制：模型必須能放入單一裝置！\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 模擬管線平行化\n",
    "pipeline_stats = simulate_pipeline_parallelism(model_layers, batch_size, num_devices, num_microbatches)\n",
    "\n",
    "print(\"管線平行化（GPipe）：\")\n",
    "print(f\"  配置：{num_devices} 個裝置，{num_microbatches} 個微批次\")\n",
    "print(f\"  前向時間：       {pipeline_stats['forward']:.1f} 單位\")\n",
    "print(f\"  反向時間：       {pipeline_stats['backward']:.1f} 單位\")\n",
    "print(f\"  通訊時間：       {pipeline_stats['communication']:.1f} 單位（無！）\")\n",
    "print(f\"  總時間：         {pipeline_stats['total']:.1f} 單位\")\n",
    "print(f\"  效率：           {pipeline_stats['efficiency']*100:.1f}%\")\n",
    "print(f\"  氣泡比例：       {pipeline_stats['bubble_fraction']*100:.1f}%\")\n",
    "print(f\"  ✓ 優勢：可以訓練 {num_devices}× 更大的模型！\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n關鍵差異：\")\n",
    "print(\"  • 資料平行：快速，但模型必須能放入單一裝置\")\n",
    "print(\"  • 管線平行：可以訓練巨型模型\")\n",
    "print(\"  • GPipe：沒有通訊開銷（不像資料平行）\")\n",
    "print(\"  • 權衡：管線有氣泡時間，資料平行有通訊開銷\")\n",
    "\n",
    "print(\"\\n✓ 比較完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第八節：完整的 GPipe 訓練迴圈\n",
    "\n",
    "讓我們把所有內容整合起來：使用 GPipe 的完整訓練迴圈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(outputs: List[np.ndarray], labels: List[np.ndarray]) -> float:\n",
    "    \"\"\"計算跨微批次的平均損失（簡化的 MSE）。\"\"\"\n",
    "    total_loss = 0.0\n",
    "    for output, label in zip(outputs, labels):\n",
    "        total_loss += np.mean((output - label) ** 2)\n",
    "    return total_loss / len(outputs)\n",
    "\n",
    "\n",
    "def train_gpipe_epoch(pipeline: GPipePipelineWithRemat,\n",
    "                     X_train: np.ndarray,\n",
    "                     y_train: np.ndarray,\n",
    "                     batch_size: int,\n",
    "                     num_microbatches: int,\n",
    "                     learning_rate: float) -> List[float]:\n",
    "    \"\"\"使用 GPipe 訓練一個 epoch。\n",
    "    \n",
    "    返回：\n",
    "        每個小批次的損失列表\n",
    "    \"\"\"\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        # 獲取小批次\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        # 分割為微批次\n",
    "        microbatches = split_into_microbatches(X_batch, y_batch, num_microbatches)\n",
    "        \n",
    "        # 前向傳遞\n",
    "        outputs, boundary_inputs = pipeline.forward_pipeline_remat(microbatches)\n",
    "        \n",
    "        # 計算損失\n",
    "        labels = [mb[1] for mb in microbatches]\n",
    "        loss = compute_loss(outputs, labels)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # 反向傳遞\n",
    "        all_gradients = pipeline.backward_pipeline_remat(outputs, labels, boundary_inputs)\n",
    "        \n",
    "        # 累積梯度\n",
    "        accumulated_grads = accumulate_gradients(all_gradients)\n",
    "        \n",
    "        # 更新參數\n",
    "        apply_gradients(pipeline.partitions, accumulated_grads, learning_rate)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# 生成合成資料集\n",
    "print(\"建立合成資料集...\\n\")\n",
    "\n",
    "num_train = 256\n",
    "input_dim = 128\n",
    "output_dim = 10\n",
    "\n",
    "X_train = np.random.randn(num_train, input_dim)\n",
    "y_train_labels = np.random.randint(0, output_dim, num_train)\n",
    "y_train = np.eye(output_dim)[y_train_labels]\n",
    "\n",
    "print(f\"資料集：{num_train} 個樣本，輸入維度 {input_dim}，輸出維度 {output_dim}\")\n",
    "\n",
    "# 建立新模型和管線\n",
    "print(\"\\n初始化 GPipe 模型...\")\n",
    "\n",
    "layer_dims = [input_dim] + [256] * 10 + [output_dim]\n",
    "activations = ['relu'] * 10 + ['linear']\n",
    "model_layers = create_model(layer_dims, activations)\n",
    "\n",
    "K = 4\n",
    "partitions = partition_model(model_layers, K)\n",
    "pipeline = GPipePipelineWithRemat(partitions)\n",
    "\n",
    "print(f\"  模型：{len(model_layers)} 層\")\n",
    "print(f\"  分區：{K} 個裝置\")\n",
    "\n",
    "# 訓練配置\n",
    "batch_size = 32\n",
    "num_microbatches = 8\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "print(f\"\\n訓練配置：\")\n",
    "print(f\"  批次大小：{batch_size}\")\n",
    "print(f\"  微批次數：{num_microbatches}\")\n",
    "print(f\"  學習率：{learning_rate}\")\n",
    "print(f\"  Epochs：{num_epochs}\")\n",
    "\n",
    "# 訓練\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"訓練 GPipe 模型...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pipeline.events = []  # 重置此 epoch 的事件\n",
    "    \n",
    "    losses = train_gpipe_epoch(pipeline, X_train, y_train, \n",
    "                               batch_size, num_microbatches, learning_rate)\n",
    "    \n",
    "    avg_loss = np.mean(losses)\n",
    "    all_losses.extend(losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}：平均損失 = {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n✓ 訓練完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第九節：視覺化和分析\n",
    "\n",
    "讓我們建立 GPipe 效能的綜合視覺化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 1：訓練損失曲線\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 圖 1：訓練損失\n",
    "ax = axes[0, 0]\n",
    "ax.plot(all_losses, linewidth=2, color='darkblue')\n",
    "ax.set_xlabel('小批次', fontsize=11)\n",
    "ax.set_ylabel('損失', fontsize=11)\n",
    "ax.set_title('GPipe 訓練損失', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 圖 2：氣泡比例 vs M（微批次數）\n",
    "ax = axes[0, 1]\n",
    "M_range = np.arange(1, 65)\n",
    "K_values_plot = [2, 4, 8, 16]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "for K_val, color in zip(K_values_plot, colors):\n",
    "    bubbles = [compute_bubble_fraction(K_val, M) for M in M_range]\n",
    "    ax.plot(M_range, bubbles, label=f'K={K_val}', linewidth=2, color=color)\n",
    "\n",
    "ax.set_xlabel('微批次數 (M)', fontsize=11)\n",
    "ax.set_ylabel('氣泡比例', fontsize=11)\n",
    "ax.set_title('氣泡時間 vs 微批次數', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# 圖 3：重新實體化的記憶體節省\n",
    "ax = axes[1, 0]\n",
    "K_range = np.arange(2, 17)\n",
    "layers_per_partition = 3\n",
    "M_fixed = 8\n",
    "activation_size_mb = 10\n",
    "\n",
    "mem_without_remat = [estimate_memory_usage(M_fixed, K_val, layers_per_partition, \n",
    "                                            activation_size_mb, False) \n",
    "                     for K_val in K_range]\n",
    "mem_with_remat = [estimate_memory_usage(M_fixed, K_val, layers_per_partition, \n",
    "                                        activation_size_mb, True) \n",
    "                  for K_val in K_range]\n",
    "\n",
    "ax.plot(K_range, mem_without_remat, label='無重新實體化', linewidth=2, \n",
    "        marker='o', color='red', markersize=6)\n",
    "ax.plot(K_range, mem_with_remat, label='有重新實體化', linewidth=2, \n",
    "        marker='s', color='green', markersize=6)\n",
    "ax.set_xlabel('分區數 (K)', fontsize=11)\n",
    "ax.set_ylabel('記憶體 (MB)', fontsize=11)\n",
    "ax.set_title('記憶體使用量：重新實體化的影響', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 圖 4：管線效率 vs 配置\n",
    "ax = axes[1, 1]\n",
    "M_configs = [4, 8, 16, 32]\n",
    "K_configs = np.arange(2, 17)\n",
    "\n",
    "for M_val in M_configs:\n",
    "    efficiencies = [1 - compute_bubble_fraction(K_val, M_val) for K_val in K_configs]\n",
    "    ax.plot(K_configs, efficiencies, label=f'M={M_val}', linewidth=2, marker='o', markersize=5)\n",
    "\n",
    "ax.set_xlabel('裝置數 (K)', fontsize=11)\n",
    "ax.set_ylabel('管線效率', fontsize=11)\n",
    "ax.set_title('管線效率 vs 配置', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ 視覺化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第十節：關鍵洞見和現代擴展\n",
    "\n",
    "## GPipe 總結\n",
    "\n",
    "### 核心思想\n",
    "1. **管線平行化**：按層將模型分割到各裝置\n",
    "2. **微批次處理**：分割小批次以減少氣泡時間\n",
    "3. **重新實體化**：用計算換取記憶體效率\n",
    "4. **F-then-B 排程**：先前向所有微批次，再反向所有\n",
    "\n",
    "### 數學洞見\n",
    "\n",
    "**氣泡比例**：\n",
    "$$\\text{Bubble} = \\frac{K-1}{K-1+M}$$\n",
    "\n",
    "**記憶體節省**（有重新實體化）：\n",
    "$$\\text{Memory}_{\\text{remat}} = \\frac{K}{L} \\times \\text{Memory}_{\\text{standard}}$$\n",
    "\n",
    "其中 L = 總層數，K = 分區數。\n",
    "\n",
    "**加速比**（相比單一裝置）：\n",
    "$$\\text{Speedup} \\approx \\frac{K}{1 + \\frac{K-1}{M}}$$\n",
    "\n",
    "### 何時使用 GPipe\n",
    "\n",
    "**使用 GPipe 當**：\n",
    "- 模型無法放入單一裝置\n",
    "- 序列化模型結構（各層）\n",
    "- 裝置間頻寬有限\n",
    "- 可以使用大的 M（許多微批次）\n",
    "\n",
    "**避免 GPipe 當**：\n",
    "- 模型能放入單一裝置（改用資料平行）\n",
    "- M 很小（氣泡時間佔主導）\n",
    "- 非序列化架構（例如，大量跳躍連接）\n",
    "\n",
    "---\n",
    "\n",
    "## 現代擴展\n",
    "\n",
    "### 1. PipeDream（Harlap et al., 2018）\n",
    "- **1F1B 排程**：交錯前向和反向\n",
    "- 減少管線深度\n",
    "- 更好的記憶體效率\n",
    "\n",
    "### 2. Megatron-LM（Shoeybi et al., 2019）\n",
    "- 結合管線 + 張量平行化\n",
    "- 水平分割層（層內）\n",
    "- 用於 5300 億參數的模型\n",
    "\n",
    "### 3. ZeRO（Rajbhandari et al., 2020）\n",
    "- 分區優化器狀態、梯度、參數\n",
    "- 補充管線平行化\n",
    "- 減少記憶體而無需複製\n",
    "\n",
    "### 4. Varuna（Athlur et al., 2022）\n",
    "- 自動管線排程優化\n",
    "- 自適應微批次處理\n",
    "- 處理異質裝置\n",
    "\n",
    "---\n",
    "\n",
    "## 實際考量\n",
    "\n",
    "### 最佳 M（微批次數）\n",
    "- **太小**：高氣泡比例\n",
    "- **太大**：微批次管理開銷\n",
    "- **經驗法則**：M ≈ 4×K\n",
    "\n",
    "### 分區策略\n",
    "- 均勻：每裝置相同層數\n",
    "- 平衡：每裝置相同計算時間\n",
    "- 記憶體感知：平衡記憶體使用\n",
    "\n",
    "### 批次大小\n",
    "- 大批次提高管線利用率\n",
    "- 但可能影響泛化能力\n",
    "- 用學習率縮放來補償\n",
    "\n",
    "---\n",
    "\n",
    "## 與其他論文的關聯\n",
    "\n",
    "**論文 5（最優腦損傷）**：剪枝減少模型大小 → 需要更少管線階段\n",
    "\n",
    "**論文 23（MDL）**：模型複雜度 vs 資料擬合 → 選擇 K（分區數）涉及權衡\n",
    "\n",
    "**論文 14（神經架構搜索）**：可以使用 GPipe 搜索對單一裝置來說太大的架構\n",
    "\n",
    "---\n",
    "\n",
    "## 實際影響\n",
    "\n",
    "GPipe 實現了：\n",
    "- **AmoebaNet-B**：5.57 億參數（比之前最好的大 8 倍）\n",
    "- **在 ImageNet 上訓練**，達到 84.4% top-1 準確率\n",
    "- **GPT-3**：1750 億參數（結合多種技術包括管線平行化）\n",
    "- **大型語言模型**：現代 LLM 使用管線 + 張量 + 資料平行化\n",
    "\n",
    "---\n",
    "\n",
    "**GPipe 的遺產**：展示了**模型平行化是可行的**，為訓練數千億參數的模型鋪平了道路。結合張量平行化和 ZeRO，它構成了現代大規模訓練的基礎！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終示範：展示 K 和 M 之間的權衡\n",
    "print(\"=\"*70)\n",
    "print(\"GPipe 配置指南\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. 選擇 K（裝置數）：\")\n",
    "print(\"   • 受限於：可用加速器數量\")\n",
    "print(\"   • 更多 K = 可以訓練更大模型\")\n",
    "print(\"   • 更多 K = 更多氣泡時間（需要更大 M 來補償）\")\n",
    "\n",
    "print(\"\\n2. 選擇 M（微批次數）：\")\n",
    "print(\"   • 經驗法則：M ≈ 4×K\")\n",
    "print(\"   • 更大 M = 更少氣泡時間\")\n",
    "print(\"   • 更大 M = 更多開銷\")\n",
    "print(\"   • 必須能整除批次大小\")\n",
    "\n",
    "print(\"\\n3. 配置範例：\")\n",
    "configs = [\n",
    "    (2, 8, 32),\n",
    "    (4, 16, 64),\n",
    "    (8, 32, 128),\n",
    "    (16, 64, 256),\n",
    "]\n",
    "\n",
    "for K, M, batch in configs:\n",
    "    bubble = compute_bubble_fraction(K, M)\n",
    "    efficiency = 1 - bubble\n",
    "    print(f\"   K={K:2d}, M={M:2d}, batch={batch:3d} → \"\n",
    "          f\"效率={efficiency*100:.1f}%, 氣泡={bubble*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ GPipe 實作完成！\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
