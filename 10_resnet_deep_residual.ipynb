{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文 10：用於圖像識別的深度殘差學習\n",
    "## Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015)\n",
    "\n",
    "### ResNet：跳躍連接使非常深的網路成為可能\n",
    "\n",
    "ResNet 引入了殘差連接，使得訓練超過 100 層的網路成為可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題：深度網路的退化\n",
    "\n",
    "在 ResNet 之前，添加更多層實際上會使網路變差（不是因為過擬合，而是優化困難）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "class PlainLayer:\n",
    "    \"\"\"標準神經網路層\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.randn(output_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z = np.dot(self.W, x) + self.b\n",
    "        self.a = relu(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        da = dout * relu_derivative(self.z)\n",
    "        self.dW = np.dot(da, self.x.T)\n",
    "        self.db = np.sum(da, axis=1, keepdims=True)\n",
    "        dx = np.dot(self.W.T, da)\n",
    "        return dx\n",
    "\n",
    "class ResidualBlock:\n",
    "    \"\"\"帶跳躍連接的殘差區塊：y = F(x) + x\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.layer1 = PlainLayer(size, size)\n",
    "        self.layer2 = PlainLayer(size, size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "        # 殘差路徑 F(x)\n",
    "        out = self.layer1.forward(x)\n",
    "        out = self.layer2.forward(out)\n",
    "        \n",
    "        # 跳躍連接：F(x) + x\n",
    "        self.out = out + x\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # 梯度通過兩條路徑流動\n",
    "        # 跳躍連接提供直接路徑\n",
    "        dx_residual = self.layer2.backward(dout)\n",
    "        dx_residual = self.layer1.backward(dx_residual)\n",
    "        \n",
    "        # 總梯度：殘差路徑 + 跳躍連接\n",
    "        dx = dx_residual + dout  # 這是關鍵！\n",
    "        return dx\n",
    "\n",
    "print(\"ResNet 元件已初始化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構普通網路 vs ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainNetwork:\n",
    "    \"\"\"沒有跳躍連接的普通深度網路\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        self.layers = []\n",
    "        \n",
    "        # 第一層\n",
    "        self.layers.append(PlainLayer(input_size, hidden_size))\n",
    "        \n",
    "        # 隱藏層\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PlainLayer(hidden_size, hidden_size))\n",
    "        \n",
    "        # 輸出層\n",
    "        self.layers.append(PlainLayer(hidden_size, input_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "class ResidualNetwork:\n",
    "    \"\"\"帶殘差連接的深度網路\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_blocks):\n",
    "        # 投影到隱藏大小\n",
    "        self.input_proj = PlainLayer(input_size, hidden_size)\n",
    "        \n",
    "        # 殘差區塊\n",
    "        self.blocks = [ResidualBlock(hidden_size) for _ in range(num_blocks)]\n",
    "        \n",
    "        # 投影回輸出\n",
    "        self.output_proj = PlainLayer(hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj.forward(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = self.output_proj.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.output_proj.backward(dout)\n",
    "        for block in reversed(self.blocks):\n",
    "            dout = block.backward(dout)\n",
    "        dout = self.input_proj.backward(dout)\n",
    "        return dout\n",
    "\n",
    "# 建立網路\n",
    "input_size = 16\n",
    "hidden_size = 16\n",
    "depth = 10\n",
    "\n",
    "plain_net = PlainNetwork(input_size, hidden_size, depth)\n",
    "resnet = ResidualNetwork(input_size, hidden_size, depth)\n",
    "\n",
    "print(f\"建立了 {depth} 層的普通網路\")\n",
    "print(f\"建立了 {depth} 個殘差區塊的 ResNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 展示梯度流動\n",
    "\n",
    "關鍵優勢：梯度通過跳躍連接更容易流動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_gradient_flow(network, name):\n",
    "    \"\"\"測量不同深度的梯度大小\"\"\"\n",
    "    # 隨機輸入\n",
    "    x = np.random.randn(input_size, 1)\n",
    "    \n",
    "    # 前向傳遞\n",
    "    output = network.forward(x)\n",
    "    \n",
    "    # 建立梯度信號\n",
    "    dout = np.ones_like(output)\n",
    "    \n",
    "    # 反向傳遞\n",
    "    network.backward(dout)\n",
    "    \n",
    "    # 收集梯度大小\n",
    "    grad_norms = []\n",
    "    \n",
    "    if isinstance(network, PlainNetwork):\n",
    "        for layer in network.layers:\n",
    "            grad_norm = np.linalg.norm(layer.dW)\n",
    "            grad_norms.append(grad_norm)\n",
    "    else:  # ResNet\n",
    "        grad_norms.append(np.linalg.norm(network.input_proj.dW))\n",
    "        for block in network.blocks:\n",
    "            grad_norm1 = np.linalg.norm(block.layer1.dW)\n",
    "            grad_norm2 = np.linalg.norm(block.layer2.dW)\n",
    "            grad_norms.append(np.mean([grad_norm1, grad_norm2]))\n",
    "        grad_norms.append(np.linalg.norm(network.output_proj.dW))\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# 測量兩個網路的梯度流動\n",
    "plain_grads = measure_gradient_flow(plain_net, \"普通網路\")\n",
    "resnet_grads = measure_gradient_flow(resnet, \"ResNet\")\n",
    "\n",
    "# 繪製比較\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(plain_grads)), plain_grads, 'o-', label='普通網路', linewidth=2)\n",
    "plt.plot(range(len(resnet_grads)), resnet_grads, 's-', label='ResNet', linewidth=2)\n",
    "plt.xlabel('層深度（越深 →）')\n",
    "plt.ylabel('梯度大小')\n",
    "plt.title('梯度流動：ResNet vs 普通網路')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n普通網路 - 第一層梯度：{plain_grads[0]:.6f}\")\n",
    "print(f\"普通網路 - 最後層梯度：{plain_grads[-1]:.6f}\")\n",
    "print(f\"梯度比（第一/最後）：{plain_grads[0]/plain_grads[-1]:.2f}x\\n\")\n",
    "\n",
    "print(f\"ResNet - 第一層梯度：{resnet_grads[0]:.6f}\")\n",
    "print(f\"ResNet - 最後層梯度：{resnet_grads[-1]:.6f}\")\n",
    "print(f\"梯度比（第一/最後）：{resnet_grads[0]/resnet_grads[-1]:.2f}x\")\n",
    "\n",
    "print(f\"\\nResNet 維持梯度流動好 {(plain_grads[0]/plain_grads[-1]) / (resnet_grads[0]/resnet_grads[-1]):.1f} 倍！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覺化學習到的表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成合成的類圖像資料\n",
    "def generate_patterns(num_samples=100, size=8):\n",
    "    \"\"\"生成簡單的 2D 模式\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        pattern = np.zeros((size, size))\n",
    "        \n",
    "        if i % 3 == 0:\n",
    "            # 水平線\n",
    "            pattern[2:3, :] = 1\n",
    "            label = 0\n",
    "        elif i % 3 == 1:\n",
    "            # 垂直線\n",
    "            pattern[:, 3:4] = 1\n",
    "            label = 1\n",
    "        else:\n",
    "            # 對角線\n",
    "            np.fill_diagonal(pattern, 1)\n",
    "            label = 2\n",
    "        \n",
    "        # 添加雜訊\n",
    "        pattern += np.random.randn(size, size) * 0.1\n",
    "        \n",
    "        X.append(pattern.flatten())\n",
    "        y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = generate_patterns(num_samples=30, size=4)\n",
    "\n",
    "# 視覺化範例模式\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    sample = X[i].reshape(4, 4)\n",
    "    ax.imshow(sample, cmap='gray')\n",
    "    ax.set_title(f'模式類型 {y[i]}')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"生成了 {len(X)} 個模式樣本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恆等映射：核心洞見\n",
    "\n",
    "**關鍵洞見**：如果恆等映射是最優的，殘差應該學習 F(x) = 0，這比學習 H(x) = x 更容易"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示恆等映射\n",
    "x = np.random.randn(hidden_size, 1)\n",
    "\n",
    "# 初始化殘差區塊\n",
    "block = ResidualBlock(hidden_size)\n",
    "\n",
    "# 如果權重接近零，F(x) ≈ 0\n",
    "block.layer1.W *= 0.001\n",
    "block.layer2.W *= 0.001\n",
    "\n",
    "# 前向傳遞\n",
    "output = block.forward(x)\n",
    "\n",
    "# 檢查輸出是否約等於輸入（恆等）\n",
    "identity_error = np.linalg.norm(output - x)\n",
    "\n",
    "print(\"恆等映射展示：\")\n",
    "print(f\"輸入範數：{np.linalg.norm(x):.4f}\")\n",
    "print(f\"輸出範數：{np.linalg.norm(output):.4f}\")\n",
    "print(f\"恆等誤差 ||F(x) + x - x||：{identity_error:.6f}\")\n",
    "print(f\"\\n當權重接近零時，殘差區塊 ≈ 恆等函數！\")\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.flatten(), 'o-', label='輸入 x', alpha=0.7)\n",
    "plt.plot(output.flatten(), 's-', label='輸出 (x + F(x))', alpha=0.7)\n",
    "plt.xlabel('維度')\n",
    "plt.ylabel('值')\n",
    "plt.title('恆等映射：輸出 ≈ 輸入')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residual = output - x\n",
    "plt.bar(range(len(residual)), residual.flatten())\n",
    "plt.xlabel('維度')\n",
    "plt.ylabel('殘差 F(x)')\n",
    "plt.title('學習到的殘差 ≈ 0')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比較不同網路深度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_depth_scaling():\n",
    "    \"\"\"測試梯度流動如何隨深度變化\"\"\"\n",
    "    depths = [5, 10, 20, 30, 40]\n",
    "    plain_ratios = []\n",
    "    resnet_ratios = []\n",
    "    \n",
    "    for depth in depths:\n",
    "        # 建立網路\n",
    "        plain = PlainNetwork(input_size, hidden_size, depth)\n",
    "        res = ResidualNetwork(input_size, hidden_size, depth)\n",
    "        \n",
    "        # 測量梯度\n",
    "        plain_grads = measure_gradient_flow(plain, \"Plain\")\n",
    "        res_grads = measure_gradient_flow(res, \"ResNet\")\n",
    "        \n",
    "        # 計算比率（第一層/最後層梯度）\n",
    "        plain_ratio = plain_grads[0] / (plain_grads[-1] + 1e-10)\n",
    "        res_ratio = res_grads[0] / (res_grads[-1] + 1e-10)\n",
    "        \n",
    "        plain_ratios.append(plain_ratio)\n",
    "        resnet_ratios.append(res_ratio)\n",
    "    \n",
    "    # 繪製\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(depths, plain_ratios, 'o-', label='普通網路', linewidth=2, markersize=8)\n",
    "    plt.plot(depths, resnet_ratios, 's-', label='ResNet', linewidth=2, markersize=8)\n",
    "    plt.xlabel('網路深度')\n",
    "    plt.ylabel('梯度比（第一/最後層）')\n",
    "    plt.title('梯度流動隨深度的退化')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n梯度比（第一/最後）- 越高 = 梯度流動越差：\")\n",
    "    for i, d in enumerate(depths):\n",
    "        print(f\"深度 {d:2d}：普通={plain_ratios[i]:8.2f}，ResNet={resnet_ratios[i]:6.2f} \"\n",
    "              f\"（ResNet 好 {plain_ratios[i]/resnet_ratios[i]:.1f} 倍）\")\n",
    "\n",
    "test_depth_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵要點\n",
    "\n",
    "### 退化問題：\n",
    "- 給普通網路添加更多層會損害效能\n",
    "- **不是**因為過擬合（訓練誤差也增加）\n",
    "- 是因為優化困難：梯度消失/爆炸\n",
    "\n",
    "### ResNet 解決方案：跳躍連接\n",
    "```\n",
    "y = F(x, {Wi}) + x\n",
    "```\n",
    "\n",
    "**不再學習**：H(x) = 期望映射  \n",
    "**而是學習殘差**：F(x) = H(x) - x  \n",
    "**然後**：H(x) = F(x) + x\n",
    "\n",
    "### 為什麼有效：\n",
    "1. **恆等映射更容易**：如果最優映射是恆等，學習 F(x) = 0 比學習 H(x) = x 更容易\n",
    "2. **梯度高速公路**：跳躍連接提供直接的梯度路徑\n",
    "3. **加性梯度流動**：梯度同時通過殘差和跳躍路徑\n",
    "4. **無額外參數**：跳躍連接無需參數\n",
    "\n",
    "### 影響：\n",
    "- 使 152 層網路成為可能（之前限制為約 20 層）\n",
    "- 贏得 ImageNet 2015（3.57% top-5 錯誤率）\n",
    "- 成為標準架構模式\n",
    "- 啟發了變體：DenseNet、ResNeXt 等\n",
    "\n",
    "### 數學洞見：\n",
    "損失 L 對前面層的梯度：\n",
    "```\n",
    "∂L/∂x = ∂L/∂y * (∂F/∂x + ∂x/∂x) = ∂L/∂y * (∂F/∂x + I)\n",
    "```\n",
    "`+ I` 項確保梯度始終能流動！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
